{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Using the Frobenius Norm\n",
    "\n",
    "In a neuron network, the weight matrix $W^{\\ell}$ for a layer $\\ell$ has the following dimensions:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{W^{\\ell}}_{n^{\\ell} \\times n^{\\ell - 1}}\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "* $n^{\\ell}$ is the number of neurons or units in the current layer $\\ell$ \n",
    "* $n^{\\ell - 1}$ is the number of neurons or units in the input layer $\\ell - 1$ for the current layer $\\ell$\n",
    "\n",
    "The bias vector $\\vec{b}^{\\ell}$ for a layer $\\ell$ has the following dimensions:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\vec{b}^{\\ell}}_{n^{\\ell} \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "The cost function of a neural network is a function of all parameters:\n",
    "\n",
    "\\begin{align*}\n",
    "J(W^{1}, \\vec{b}^{1}, W^{2}, \\vec{b}^{2}, ..., W^{\\mathcal{L}}, \\vec{b}^{\\mathcal{L}}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{i}, y^{i})\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathcal{L}$ is the number of layers in the network. In words, the cost function is the sum of the losses over all $m$ training example scaled by a factor $\\frac{1}{m}$. To regularize the weights, we add an additional term to the equation as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "J_{\\text{regularized}}(W^{1}, \\vec{b}^{1}, W^{2}, \\vec{b}^{2}, ..., W^{\\mathcal{L}}, \\vec{b}^{\\mathcal{L}}) = \\frac{1}{m}\\sum^{m}_{i=1}L(\\hat{y}^{i}, y^{i}) + \\frac{\\lambda}{2m}\\sum^{\\mathcal{L}}_{\\ell=1}||W^{\\ell}||^{2}\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "* $||W^{\\ell}||^{2}=\\sum^{n^{\\ell}}_{i=1}\\sum^{\\ell - 1}_{j=1} (W^{\\ell}_{ij})^2$ is Frobenius norm of the $\\ell^{th}$ layer weight matrix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent Update Rule with Regularization\n",
    "\n",
    "From backpropagation, we obtain the partial derivatives of the cost function w.r.t each of the weight matrices plus the regularization term:\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial{J}}{\\partial{W^{\\ell}}} + \\frac{\\lambda}{m}W^{\\ell}\n",
    "\\end{align*}\n",
    "\n",
    "The update rule is then:\n",
    "\n",
    "\\begin{align*}\n",
    "W^{\\ell} &:= W^{\\ell} - \\alpha \\big(\\frac{\\partial{J}}{\\partial{W^{\\ell}}} + \\frac{\\lambda}{m}W^{\\ell}\\big) \\\\\n",
    "&:=W^{\\ell} - \\alpha \\frac{\\partial{J}}{\\partial{W^{\\ell}}} \\textcolor{blue}{- \\frac{\\alpha \\lambda}{m}W^{\\ell}} \\\\\n",
    "&:=W^{\\ell} \\textcolor{blue}{- \\frac{\\alpha \\lambda}{m}W^{\\ell}} - \\alpha \\frac{\\partial{J}}{\\partial{W^{\\ell}}} \\\\\n",
    "&:=\\textcolor{blue}{(1 - \\frac{\\alpha \\lambda}{m})}W^{\\ell} - \\alpha \\frac{\\partial{J}}{\\partial{W^{\\ell}}} \\\\\n",
    "\\end{align*}\n",
    "\n",
    "This is known as \"weight decay\" since, at each step of the algorithm, the matrix $W^{\\ell}$ is shrunk by a factor that is smaller than 1. This factor is the term highlighted in blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The regularization technique formulated above can be implemented in Keras by using the kernel_regularizer parameter of the Dense layer\n",
    "model_l2_regularize = Sequential(\n",
    "    [               \n",
    "        Input(shape=(400,)),   \n",
    "        Dense(units=25, activation=\"relu\", kernel_regularizer=regularizers.L2(l2=0.01),  use_bias=True, name='layer_1'), # Lambda = 0.01 \n",
    "        Dense(units=15, activation=\"relu\", kernel_regularizer=regularizers.L2(l2=0.03), use_bias=True, name='layer_2'), # Lambda = 0.03\n",
    "        Dense(units=1, activation=\"sigmoid\", use_bias=True, name='output_layer')\n",
    "    ], name = \"my_model\" \n",
    ")   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use other types of regularization:\n",
    "\n",
    "* L1 regularization `regularizers.l1(l1=0.001)`: The cost added is proportional to the absolute value of the weight coefficients (the L1 norm of the weights)\n",
    "* L2 regularization `regularizers.l1_l2(l1=0.001, l2=0.001)`: The cost added is proportional to the square of the value of the weight coefficients (the L2 norm of the weights).\n",
    "* Using both L1 and L2 simultaneously.\n",
    "\n",
    "The lambda values `l1` and `l2` are hyperparameters that we can tune."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverted Drop-out Regularization\n",
    "\n",
    "Dropout, applied to a layer (usually hidden layers), consists of randomly dropping out (setting to zero) a number of activation values of the layer during training. Suppose a given layer would normally return an activation vector of `[0.2, 0.5, 1.3, 0.8, 1.1]` for a given input training example during training. After applying dropout, this vector will have a few zero entries distributed at random: for example, `[0, 0.5, 1.3, 0, 1.1]`. The **dropout rate** is the fraction of the features that are zeroed out; it’s usually set between 0.2 and 0.5. At test time, no units are dropped out; instead, the layer’s output values are scaled down by a factor equal to the dropout rate to balance for the fact that more units are active at test time than at training time.\n",
    "\n",
    "\n",
    "```python\n",
    "# Multiply (element-wise) the activation matrix by a matrix of 0's & 1' with the same dimensions, \n",
    "# Elements that are multiplied by 0's are then 'zeroed-out'\n",
    "layer_output *= np.random.rand(*layer_output.shape) < keep_prob\n",
    "# Scale down by factor of 'keep_prob'\n",
    "layer_output /= keep_prob\n",
    "```\n",
    "\n",
    "<center> <img  src=\"images/drop_out.png\" width=\"600\" />   </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dropout = Sequential(\n",
    "    [               \n",
    "        Input(shape=(400,)),   \n",
    "        Dense(units=25, activation=\"relu\", kernel_constraint=MaxNorm(max_value=2, axis=0),  use_bias=True, name='layer_1'), \n",
    "        # Applies to the output activation values of the layer right above 'layer_1'\n",
    "        Dropout(rate=0.5, name='dropout_layer_1'),\n",
    "        Dense(units=15, activation=\"relu\", kernel_regularizer=MaxNorm(max_value=3, axis=0), use_bias=True, name='layer_2'), \n",
    "        # Applies to the output activation values of the layer right above 'layer_2'\n",
    "        Dropout(rate=0.2, name='dropout_layer_2'),\n",
    "        Dense(units=1, activation=\"sigmoid\", use_bias=True, name='output_layer')\n",
    "    ], name = \"my_model\" \n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_1 (Dense)             (None, 25)                10025     \n",
      "                                                                 \n",
      " dropout_layer_1 (Dropout)   (None, 25)                0         \n",
      "                                                                 \n",
      " layer_2 (Dense)             (None, 15)                390       \n",
      "                                                                 \n",
      " dropout_layer_2 (Dropout)   (None, 15)                0         \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,431\n",
      "Trainable params: 10,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_dropout.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use of weight constraint is recommended by the literature:\n",
    "\n",
    "> Though large momentum and learning rate speed up learning, they sometimes cause the network weights to grow very large. To prevent this, we can use max-norm regularization. This constrains the norm of the vector of incoming weights at each hidden unit to be bound by a constant c. Typical values of c range from 3 to 4.\n",
    "\n",
    "[Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://jmlr.org/papers/v15/srivastava14a.html), 2014.\n",
    "\n",
    "The intuition as to why randomly removing a different subset of neurons on each example would reduce overfitting is noise. The core idea is that introducing noise in the output values of a hidden layer can break up happenstance patterns that aren’t significant (what Hinton, the originator of this technique, refers to as conspiracies), that the network will start memorizing if no noise is present."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
