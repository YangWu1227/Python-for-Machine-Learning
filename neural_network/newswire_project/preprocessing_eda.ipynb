{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive shell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Data wrangling and standard library\n",
    "import os\n",
    "from itertools import  islice\n",
    "import numpy as np\n",
    "\n",
    "# Machine learning\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "There are 46 different topics in the Reuters dataset; some topics are more represented than others, but each topic has at least 10 examples in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X, train_y), (test_X, test_y) = reuters.load_data(num_words=10000, test_split=0.2, seed=1227)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982,), (8982,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape, train_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2246,), (2246,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_X.shape, test_y.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of Training Examples\n",
    "\n",
    "Each row of training example is a list of integers (indices), and there are 8,982 of such training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list([1, 599, 127, 262, 6474, 8548, 2, 1184, 7, 10, 2021, 1027, 10, 1633, 153, 6, 676, 6, 9794, 403, 3217, 162, 172, 4, 294, 517, 237, 676, 57, 85, 136, 583, 164, 4, 517, 9, 4, 155, 1700, 403, 1082, 590, 3884, 13, 109, 206, 2, 208, 483, 854, 22, 5382, 13, 271, 99, 179, 1355, 6, 4, 214, 1574, 2854, 2886, 118, 4, 2170, 179, 718, 1440, 2, 36, 34, 1845, 10, 2066, 41, 805, 30, 625, 268, 1648, 1845, 24, 692, 164, 4, 78, 1571, 708, 4, 3884, 9, 4, 237, 33, 1310, 10, 2066, 268, 33, 646, 6, 1133, 24, 2066, 36, 8, 7, 4, 1027, 3691, 7, 521, 42, 237, 534, 6, 1773, 725, 21, 403, 3217, 162, 117, 10, 306, 555, 40, 6922, 66, 1704, 164, 4, 78, 1930, 3884, 543, 660, 1048, 1306, 2978, 2346, 4, 2886, 9, 5382, 55, 5161, 6, 1773, 799, 2310, 2071, 21, 294, 162, 6, 4, 403, 107, 129, 2066, 23, 625, 2, 8, 676, 164, 4, 78, 1520, 1207, 10, 295, 216, 161, 144, 62, 119, 190, 1085, 51, 152, 216, 23, 189, 2, 9, 137, 2277, 55, 1982, 13, 532, 3686, 1292, 3051, 6, 4, 782, 1376, 913, 36, 8, 970, 209, 351, 6, 1310, 10, 2066, 618, 3419, 2125, 36, 8, 778, 34, 905, 676, 31, 782, 126, 692, 62, 6776, 43, 10, 447, 5, 4, 4098, 184, 1383, 1290, 6205, 36, 855, 129, 783, 51, 8, 778, 2, 6, 57, 1119, 1704, 31, 782, 17, 12]),\n",
       "       list([1, 4, 990, 4004, 524, 998, 6, 1793, 10, 2462, 21, 73, 1167, 5, 659, 249, 4207, 800, 1561, 9, 100, 545, 355, 249, 9257, 1561, 4, 474, 6132, 27, 990, 387, 415, 265, 7373, 2, 75, 2, 5420, 2, 1958, 5, 659, 249, 50, 34, 1429, 6, 1602, 5012, 930, 22, 181, 96, 848, 27, 4217, 126, 355, 2479, 6, 219, 266, 97, 76, 87, 52, 998, 33, 500, 84, 73, 659, 249, 236, 4, 2, 2923, 8, 4, 249, 1496, 62, 7130, 7, 3150, 7, 807, 6, 10, 1264, 5, 659, 249, 9, 4334, 5, 266, 87, 189, 286, 65, 9, 249, 87, 55, 3675, 1888, 2, 8, 7, 10, 2075, 31, 1422, 398, 4, 474, 2378, 73, 2, 1277, 2, 30, 1429, 6, 2, 7964, 1055, 6, 9257, 1422, 58, 395, 9027, 1183, 17, 12])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First two training samples\n",
    "train_X[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mdbl', 10996),\n",
       " ('fawc', 16260),\n",
       " ('degussa', 12089),\n",
       " ('woods', 8803),\n",
       " ('hanging', 13796)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "# The items() method returns a view object, which contains the key-value pairs of the dictionary, as tuples in a list\n",
    "# The islice() method returns an iterator object used to access 'selected' (first 5) elements from the list of tuples returned by items()\n",
    "list(islice(word_index.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10999, 'mdbl'),\n",
       " (16263, 'fawc'),\n",
       " (12092, 'degussa'),\n",
       " (8806, 'woods'),\n",
       " (13799, 'hanging')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary mapping integers to words (in other words, reverse the word_index dictionary key-value pairs)\n",
    "# We add 3 to the integer indices because 0, 1, and 2 are reserved indices for \"padding\", \"start of sequence\", and \"unknown\"\n",
    "reverse_word_index = {value + 3: key for key, value in word_index.items()}\n",
    "list(islice(reverse_word_index.items(), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Key not found'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Key not found'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Key not found'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'Key not found'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'of'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'to'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(7):\n",
    "    reverse_word_index.get(i, 'Key not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'? french foreign minister jean bernard ? predicted in a published interview a successful end to negotiations to admit gulf petrochemical exports into the european community ec negotiations have been under way between the community and the six nation gulf cooperation council gcc for three years ? due here tomorrow from oman for his first official visit to the united arab emirates uae told the semi official daily al ? he was confident a solution would soon be reached i am confident that problems between the two big partners the gcc and the ec will find a solution i will work to reach that solution he said in the interview conducted in paris an ec decision to impose tariffs on gulf petrochemical exports over a set quota has strained trade relations between the two sides gcc members saudi arabia kuwait bahrain qatar the uae and oman are threatening to impose heavy customs duties on european exports to the gulf if no solution is reached ? said negotiations between the two groups took a long time because there were 20 countries involved but added time is now ? and all circumstances are appropriate for making crucial progress referring to the iran iraq war he said efforts should continue to find a solution despite prevailing difficulties he said france was continuing negotiations with iran some problems were solved as a result of the contacts while others remained unresolved he gave no details but said france ? to have normal relations with iran reuter 3'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Decode first training examples back to words\n",
    "decoded_first_train_sample = \" \".join([reverse_word_index.get(i, '?') for i in train_X[0]])\n",
    "decoded_first_train_sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Unique Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert each list of integers to numpy array\n",
    "for index, arr in np.ndenumerate(train_X):\n",
    "    train_X[index] = np.array(arr, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1,  53, 188,  26,  14, 188,  26, 255, 346, 219,  91, 142, 146,\n",
       "         93, 102,  17,  12], dtype=int32),\n",
       " array([   1,  603, 1827, 2175,    7,  104,  138,  165,   47,   20,   22,\n",
       "          10,   29,  157,    6, 1890, 3200,    4,    2, 4494,   29,   21,\n",
       "          29,  276,    4, 1167,  379,    8,  104, 1827, 2175,   62,   84,\n",
       "         158,   63,   20,   22, 1038, 5841,    7,  110,  185,   77,  202,\n",
       "         318,   47,   20,   22,   10,  139,  157,   51,  138,   83,   12,\n",
       "          20,   22,   10,   29,  157,  266, 1827, 2175,   62,  958,  208,\n",
       "           6,   10,  365,   63,   20,   29,   21,   29,  154,    7,    2,\n",
       "        1219, 2175,    6,  533, 6809,   13,    4,    2, 3242,  276, 6904,\n",
       "        1867,   27,  246,  260,  128,  140,    2,   12], dtype=int32))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[2], train_X[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1307140,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenate all training samples into one numpy array\n",
    "all_integers = np.concatenate(train_X)\n",
    "all_integers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.22856"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check memory usage in megabytes\n",
    "all_integers.size * all_integers.itemsize * 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9977"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of unique words\n",
    "np.unique(all_integers).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9999"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max value of all integers\n",
    "all_integers.max()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structure of Targets\n",
    "\n",
    "The label associated with each training example is an integer between 0 and 45, each representing a topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "        34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45]),\n",
       " (46,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(train_y), np.unique(train_y).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for Missingness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Targets\n",
    "np.isnan(train_y).any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "missingness_dict = {}\n",
    "for index, arr in np.ndenumerate(train_X):\n",
    "    if np.isnan(arr).any():\n",
    "        missingness_dict[index] = np.isnan(arr).sum() / sum(len(arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missingness_dict == {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del missingness_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No missing values for the training data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "### Training Data\n",
    "\n",
    "We one-hot encode the lists of integers to turn them into vectors of 0s and 1s. For instance, we turn the sequence [3, 5] into a 10,000-dimensional vector that would be all 0s except for indices 3 and 5, which would be 1s. Recall that the max word index is 9,999, which means that no word index will exceed 10,000:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sequences(sequences, dimension=10000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    One-hot encode training data\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences : np.ndarray\n",
    "        Training data where each of the m example or row is an array of integers\n",
    "    dimension : int, optional\n",
    "        Number of possible unique words, by default 10,000\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        Training data where each example is a one-hot encoded vector\n",
    "    \"\"\"\n",
    "    # Initialize a matrix of zeros with shape (len(sequences), dimension)\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for index, word_indices in np.ndenumerate(sequences):\n",
    "        # For each row of the zero matrix, set all column indices that equal to the word index to 1\n",
    "        # For the ith example, if the word index is [1, 2, 9, 1000, 983, 454], then set results[i, [1, 2, 9, 1000, 983, 454]] = 1.\n",
    "        results[index, word_indices] = 1.\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize training and test data\n",
    "train_X = vectorize_sequences(train_X)\n",
    "test_X = vectorize_sequences(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first ten training example\n",
    "train_X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 1., 1., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine the first ten testing example\n",
    "test_X[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Targets\n",
    "\n",
    "We use one-hot encoding for the labels, creating a matrix with 46 columns, each of which is a all-zero column vector with one's in the place of the label index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8982, 46), (2246, 46))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_y = to_categorical(train_y)\n",
    "one_hot_test_y = to_categorical(test_y)\n",
    "one_hot_train_y.shape, one_hot_test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_train_y, one_hot_test_y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compute the matrix sparsity for these matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782608695652174"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (np.count_nonzero(one_hot_train_y, axis=(0, 1)) / one_hot_train_y.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9782608695652174"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (np.count_nonzero(one_hot_test_y, axis=(0, 1)) / one_hot_test_y.size)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both matrices are very sparse."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store Data To file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/processed_data.npy', 'wb') as f:\n",
    "    np.save(f, train_X)\n",
    "    np.save(f, one_hot_train_y)\n",
    "    np.save(f, test_X)\n",
    "    np.save(f, one_hot_test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
