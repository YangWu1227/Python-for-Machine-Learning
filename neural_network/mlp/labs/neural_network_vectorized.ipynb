{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use(\"utils/deeplearning.mplstyle\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils.lab_utils_common import sigmoid\n",
    "import logging\n",
    "\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sigmoid"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original Implementation \n",
    "\n",
    "<center> <img src=\"images/numpy_nn_implementation.png\" width=\"750\" height=\"450\"> </center> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dense(a_in, W, b):\n",
    "    \"\"\"\n",
    "    Computes dense layer\n",
    "    Args:\n",
    "      a_in (ndarray (n, )) : 1 example or row of the data\n",
    "      W    (ndarray (n, j)) : weight matrix, n features (number of columns of design matrix minus intercept column) per unit, j units\n",
    "      b    (ndarray (j, )) : bias vector, j units\n",
    "    Returns\n",
    "      a_out (ndarray (j,))  : j units\n",
    "    \"\"\"\n",
    "    # Obtain the number of units (neurons) in the layer from the second axis of the weight matrix (columns)\n",
    "    # Each column of the weight matrix is a vector of weights for a unit, so j units means j columns in the weight matrix\n",
    "    units = W.shape[1]\n",
    "    # Initialize the vector of activation values to zeros\n",
    "    a_out = np.zeros(units)\n",
    "    for j in range(units):\n",
    "        w = W[\n",
    "            :, j\n",
    "        ]  # Subset the jth column of the weight matrix, a vector of weights w for the jth unit\n",
    "        z = (\n",
    "            np.dot(w, a_in) + b[j]\n",
    "        )  # Compute linear predictor z = w^T a_in + b for the jth unit\n",
    "        a_out[j] = g(\n",
    "            z\n",
    "        )  # Compute activation value (logit transformation) a_out[j] = g(z) for the jth unit\n",
    "    return a_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([200,  17]),\n",
       " array([[ 1, -3,  5],\n",
       "        [-2,  4, -6]]),\n",
       " array([-1,  1,  2]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array([200, 17])\n",
    "W = np.array([[1, -3, 5], [-2, 4, -6]])\n",
    "b = np.array([-1, 1, 2])\n",
    "\n",
    "x, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00e+000, 7.12e-218, 1.00e+000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dense(x, W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Implementation\n",
    "\n",
    "Instead of for loop\n",
    "\n",
    "```python\n",
    "a_out = np.zeros(units)\n",
    "for j in range(units):               \n",
    "    w = W[:,j]                       \n",
    "    z = np.dot(w, a_in) + b[j]       \n",
    "    a_out[j] = g(z) \n",
    "```\n",
    "\n",
    "which computes and logit transforms the linear predictor for each neuron $j$ in the layer sequentially, we can use matrix multiplication to vectorize these computations. Note that the following computations are equivalent. Suppose we have the following layers and training data:\n",
    "\n",
    "* layer $l$ with $j$ neuron units\n",
    "* layer $l-1$ with $n$ neuron units\n",
    "\n",
    "\\begin{align*}\n",
    "a^{l}_{1} &= g(\\underbrace{\\mathbf{w}^{l}_{1}}_{n \\times 1} \\cdot \\underbrace{a^{l-1}}_{n \\times 1} + b^{l}_{1}) = \n",
    "g(\\underbrace{(\\vec{a}^{l-1})^{T}}_{1 \\times n}\\underbrace{\\mathbf{w}^{l}_{1}}_{n\\times1} + b^{l}_{1}) \\\\\n",
    "a^{l}_{2} &= g(\\underbrace{\\mathbf{w}^{l}_{2}}_{n \\times 1} \\cdot \\underbrace{a^{l-1}}_{n \\times 1} + b^{l}_{2}) =\n",
    "g(\\underbrace{(\\vec{a}^{l-1})^{T}}_{1 \\times n}\\underbrace{\\mathbf{w}^{l}_{2}}_{n\\times1} + b^{l}_{2}) \\\\\n",
    "& \\hspace{10mm} \\vdots \\\\\n",
    "a^{l}_{j} &= g(\\underbrace{\\mathbf{w}^{l}_{j}}_{n \\times 1} \\cdot \\underbrace{a^{l-1}}_{n \\times 1} + b^{l}_{j}) = \n",
    "g(\\underbrace{(\\vec{a}^{l-1})^{T}}_{1 \\times n}\\underbrace{\\mathbf{w}^{l}_{j}}_{n\\times1} + b^{l}_{j})\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we load the above $j$ scalars into a row vector $\\underbrace{(\\vec{a}^{l})^{T}}_{1 \\times j} = \\begin{bmatrix}a^{l}_{1} & a^{l}_{2} & \\ldots & a^{l}_{j}\\end{bmatrix}=g(\\underbrace{\\vec{z}^{l}}_{1 \\times j})$, we notice that the computations above can be first expressed as a matrix product as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\vec{z}^{l}}_{1 \\times j}&=\\textcolor{orange}{\\underbrace{a^{l-1}}_{1 \\times n}\n",
    "\\underbrace{\n",
    "\\begin{bmatrix} \n",
    "\\vert & \\vert & & \\vert  \\\\\n",
    "\\mathbf{w}^{l}_{1} & \\mathbf{w}^{l}_{2} & \\ldots & \\mathbf{w}^{l}_{j} \\\\\n",
    "\\vert & \\vert & & \\vert \n",
    "\\end{bmatrix}}_{n \\times j}} + \\underbrace{\\begin{bmatrix}b^{l}_{1} & b^{l}_{2} & \\ldots & b^{l}_{j}\\end{bmatrix}}_{1\\times j} \\\\\n",
    "&=\\underbrace{\\begin{bmatrix}a^{l-1}_{1} & a^{l-1}_{2} & \\ldots & a^{l-1}_{n}\\end{bmatrix}}_{1 \\times n}\n",
    "\\underbrace{\n",
    "\\begin{bmatrix} \n",
    "\\vert & \\vert & & \\vert  \\\\\n",
    "\\mathbf{w}^{l}_{1} & \\mathbf{w}^{l}_{2} & \\ldots & \\mathbf{w}^{l}_{j} \\\\\n",
    "\\vert & \\vert & & \\vert \n",
    "\\end{bmatrix}}_{n \\times j} + \\underbrace{\\begin{bmatrix}b^{l}_{1} & b^{l}_{2} & \\ldots & b^{l}_{j}\\end{bmatrix}}_{1\\times j} \\\\\n",
    "&=\\begin{bmatrix}\\underbrace{(\\vec{a}^{l-1})^{T}}_{1 \\times n}\\underbrace{\\mathbf{w}^{l}_{1}}_{n\\times1} + b^{l}_{1} & \\underbrace{(\\vec{a}^{l-1})^{T}}_{1 \\times n}\\underbrace{\\mathbf{w}^{l}_{2}}_{n\\times1} + b^{l}_{2} & \\ldots & \\underbrace{(\\vec{a}^{l-1})^{T}}_{1 \\times n}\\underbrace{\\mathbf{w}^{l}_{j}}_{n\\times1} + b^{l}_{j} \\end{bmatrix} \\\\\n",
    "&=\\underbrace{\\begin{bmatrix}z^{l}_{1} & z^{l}_{2} & \\ldots & z^{j}_{1}\\end{bmatrix}}_{1\\times j}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The expression highlighted in orange is the dot product between the row vector ($\\underbrace{a^{l-1}}_{1 \\times n}$) and each of the column vectors ($\\underbrace{\\mathbf{w}^{l}_{i}}_{n \\times 1}$). \n",
    "\n",
    "Then, we can apply the sigmoid function to each element of the row vector above:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{(\\vec{a}^{l})^{T}}_{1 \\times j}&=\\begin{bmatrix}g(z^{l}_{1}) & g(z^{l}_{2}) & \\ldots & g(z^{l}_{j})\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_dense_vectorized(A_in, W, B):\n",
    "    \"\"\"\n",
    "    Computes dense layer\n",
    "    Args:\n",
    "      A_in (ndarray (1, n)) : 1 example or row of the data with n features\n",
    "      W    (ndarray (n, j)) : weight matrix, n features (number of columns of design matrix minus intercept column) per unit, j units\n",
    "      B    (ndarray (1, j)) : bias raw vector with 1 row and j units (columns)\n",
    "    Returns\n",
    "      A_out (ndarray (1, j))  : j units\n",
    "    \"\"\"\n",
    "    Z = np.matmul(A_in, W) + B  # Compute linear predictor Z = a_in W + b for all units\n",
    "    A_out = g(\n",
    "        Z\n",
    "    )  # Compute activation values (logit transformation) A_out = g(Z) for all units\n",
    "    return A_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 2), (2, 3), (1, 3))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[200, 17]])\n",
    "W = np.array([[1, -3, 5], [-2, 4, -6]])\n",
    "B = np.array([[-1, 1, 2]])\n",
    "\n",
    "X.shape, W.shape, B.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00e+000, 7.12e-218, 1.00e+000]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dense_vectorized(X, W, B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between the two implementations is that the first implementation based on a for loop uses 1-D arrays (both as inputs `a_in` and `b` and output) whereas the second implementation takes as inputs 2-D arrays (matrices for all `A_in`, `B`, and `W`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('python_for_machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
