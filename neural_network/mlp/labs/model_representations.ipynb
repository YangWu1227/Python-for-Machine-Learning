{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 23:59:48.709340: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Representations for a Single Training Example\n",
    "\n",
    "In a neuron network, the weight matrix $W^{\\ell}$ for a layer $\\ell$ has the following dimensions:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{W^{\\ell}}_{n^{\\ell} \\times n^{\\ell - 1}}\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "* $n^{\\ell}$ is the number of neurons or units in the current layer $\\ell$ \n",
    "* $n^{\\ell - 1}$ is the number of neurons or units in the input layer $\\ell - 1$ for the current layer $\\ell$\n",
    "\n",
    "The bias vector $\\vec{b}^{\\ell}$ for a layer $\\ell$ has the following dimensions:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\vec{b}^{\\ell}}_{n^{\\ell} \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "Then, for any input training example $\\vec{a}^{0}$ (a vector of the training data with $n^{0}$ features) or input activation vector $\\vec{a}^{\\ell - 1}$ with $n^{\\ell - 1}$ neurons or units, we have the following matrix equation for the vector, $\\vec{z}^{\\ell}$, outputted by layer $\\ell$: \n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\vec{z^{\\ell}}}_{n^{\\ell} \\times 1} &= \\underbrace{W^{\\ell}}_{n^{\\ell} \\times n^{\\ell - 1}} \\hspace{2mm} \\underbrace{\\vec{a^{\\ell - 1}}}_{n^{\\ell - 1} \\times 1} + \\underbrace{\\vec{b}^{\\ell}}_{n^{\\ell} \\times 1} \\\\\n",
    "\\\\\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "z^{\\ell}_{1} \\\\\n",
    "\\\\\n",
    "z^{\\ell}_{2} \\\\\n",
    "\\\\\n",
    "\\vdots \\\\\n",
    "\\\\\n",
    "z^{\\ell}_{n^{\\ell}} \\end{bmatrix}}_{n^{\\ell} \\times 1}&=\\underbrace{\\begin{bmatrix} w^{\\ell}_{1,1} & w^{\\ell}_{1, 2} & w^{\\ell}_{1, 3} & \\dots & w^{\\ell}_{1, n^{\\ell - 1}} \\\\ \n",
    "\\\\\n",
    "w^{\\ell}_{2,1} & w^{\\ell}_{2, 2} & w^{\\ell}_{2, 3} & \\dots & w^{\\ell}_{2, n^{\\ell - 1}} \\\\ \n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n",
    "\\\\\n",
    "w^{\\ell}_{n^{\\ell},1} & w^{\\ell}_{n^{\\ell}, 2} & w^{\\ell}_{n^{\\ell}, 3} & \\dots & w^{\\ell}_{n^{\\ell}, n^{\\ell - 1}} \\end{bmatrix}}_{n^{\\ell} \\times n^{\\ell - 1}} \n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a^{\\ell - 1}_{1} \\\\\n",
    "\\\\\n",
    "a^{\\ell - 1}_{2} \\\\\n",
    "\\\\\n",
    "\\vdots \\\\\n",
    "\\\\\n",
    "a^{\\ell - 1}_{n^{\\ell - 1}} \\end{bmatrix}}_{n^{\\ell - 1} \\times 1} + \n",
    "\\underbrace{\\begin{bmatrix}\n",
    "b^{\\ell}_{1} \\\\\n",
    "\\\\\n",
    "b^{\\ell}_{2} \\\\\n",
    "\\\\\n",
    "\\vdots \\\\\n",
    "\\\\\n",
    "b^{\\ell}_{n^{\\ell}} \\end{bmatrix}}_{n^{\\ell} \\times 1}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where \n",
    "\n",
    "* The $i \\in 1, 2, 3, ..., n^{\\ell}$ index of the elements $w^{\\ell}_{i,j}$ indexes the $i^{th}$ neuron or unit of layer $\\ell$\n",
    "\n",
    "* The $j \\in 1, 2, 3, ..., n^{\\ell - 1}$ index of the elements $w^{\\ell}_{i,j}$ indexes the $j^{th}$ activation value of the vector $\\vec{a}^{\\ell - 1}$ outputted by layer $\\ell - 1$\n",
    "\n",
    "Then, an activation function--- linear, relu, sigmoid, tanh--- can be applied to the vector $\\vec{z}^{\\ell}$ above element-wise to obtain the final activation vector $\\vec{a}^{\\ell}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized Model Representations Using Matrices\n",
    "\n",
    "For a vectorized implementation of the neural network using matrix multiplication, we now have three matrices--- $\\underbrace{A^{0}}_{n^{0} \\times m}=\\underbrace{X}_{n^{0} \\times m}$, the weight matrix $W$, and the broadcasted vector $B^{1}$. \n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{Z^{1}}_{n^{1} \\times m}=\\underbrace{W^{1}}_{n^{1} \\times n^{0}} \\hspace{2mm} \\underbrace{X}_{n^{0} \\times m} + \\underbrace{B^{1}}_{n^{1} \\times m}\n",
    "\\end{align*}\n",
    "\n",
    "where \n",
    "\n",
    "* $n^{1}$ is the number of neurons or units in the first hidden layer\n",
    "\n",
    "* $Z^{1}$ is the matrix of values outputted by the first layer (after receiving the input training data matrix $A^{0}$ as inputs)\n",
    "\n",
    "* $A^{0}=X$ is the training data matrix **stacked horizontally** with $n^{0}$ rows (representing the features or variables) and $m$ columns (representing the training examples); in other words, each of the $m$ column vectors is an $n^{0}$-dimensional vector representing a single training example with $n^{0}$ features\n",
    "\n",
    "    - $A^{0}=X=\\begin{bmatrix} a^{0}_{1,1} & a^{0}_{1, 2} & a^{0}_{1, 3} & \\dots & a^{0}_{1, m} \\\\ \n",
    "        \\\\\n",
    "        a^{0}_{2,1} & a^{0}_{2, 2} & a^{0}_{2, 3} & \\dots & a^{0}_{2, m} \\\\ \n",
    "        \\\\\n",
    "        \\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n",
    "        \\\\\n",
    "        a^{0}_{n^{0},1} & a^{0}_{n^{0}, 2} & a^{0}_{n^{0}, 3} & \\dots & a^{0}_{n^{0}, m} \n",
    "        \\end{bmatrix}$\n",
    "\n",
    "    - The $i \\in 0, 1, 2, ..., n^{0}$ index of the elements $a^{0}_{i,j}$ indexes the $i^{th}$ **feature** of the training example among $n^{0}$ features\n",
    "\n",
    "    - The $j \\in 0, 1, 2, ..., m$ index of the elements $a^{0}_{i,j}$ indexes the $j^{th}$ training example along $m$ training examples\n",
    "\n",
    "\n",
    "\n",
    "* $B^{1}$ has column vectors that are duplicated for each of the $m$ training examples as follows:\n",
    "\n",
    "    - $\\underbrace{B^{1}}_{n^{1} \\times m}=\n",
    "        \\begin{bmatrix}\n",
    "        \\vert & \\vert &  & \\vert \\\\\n",
    "        \\vec{b}^{1}_{1} & \\vec{b}^{1}_{2} & \\dots & \\vec{b}^{1}_{m} \\\\\n",
    "        \\vert & \\vert &  & \\vert \n",
    "        \\end{bmatrix}$\n",
    "\n",
    "    - $\\vec{b}^{1}_{1}=\\vec{b}^{1}_{2}=...=\\vec{b}^{1}_{m}$ are $(n^{1} \\times 1)$ column vectors broadcasted $m$ times for all $m$ training examples \n",
    "\n",
    "In general, for any matrix $Z^{\\ell}$ outputted by layer $\\ell$, we can express the computations using the following matrix equation:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{Z^{\\ell}}_{n^{\\ell} \\times m}&=\\underbrace{W^{\\ell}}_{n^{\\ell} \\times n^{\\ell - 1}} \\hspace{2mm} \\underbrace{A^{\\ell - 1}}_{n^{\\ell - 1} \\times m} + \\underbrace{B^{\\ell}}_{n^{\\ell} \\times m} \\\\\n",
    "\\\\\n",
    "\\underbrace{\\begin{bmatrix} z^{\\ell}_{1,1} & z^{\\ell}_{1, 2} & z^{\\ell}_{1, 3} & \\dots & z^{\\ell}_{1, m} \\\\ \n",
    "\\\\\n",
    "z^{\\ell}_{2,1} & z^{\\ell}_{2, 2} & z^{\\ell}_{2, 3} & \\dots & z^{\\ell}_{2, m} \\\\ \n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n",
    "\\\\\n",
    "z^{\\ell}_{n^{\\ell},1} & z^{\\ell}_{n^{\\ell}, 2} & z^{\\ell}_{n^{\\ell}, 3} & \\dots & z^{\\ell}_{n^{\\ell}, m} \\end{bmatrix}}_{n^{\\ell}  \\times m}\n",
    "&=\\underbrace{\\begin{bmatrix} w^{\\ell}_{1,1} & w^{\\ell}_{1, 2} & w^{\\ell}_{1, 3} & \\dots & w^{\\ell}_{1, n^{\\ell - 1}} \\\\ \n",
    "\\\\\n",
    "w^{\\ell}_{2,1} & w^{\\ell}_{2, 2} & w^{\\ell}_{2, 3} & \\dots & w^{\\ell}_{2, n^{\\ell - 1}} \\\\ \n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n",
    "\\\\\n",
    "w^{\\ell}_{n^{\\ell},1} & w^{\\ell}_{n^{\\ell}, 2} & w^{\\ell}_{n^{\\ell}, 3} & \\dots & w^{\\ell}_{n^{\\ell}, n^{\\ell - 1}} \\end{bmatrix}}_{n^{\\ell} \\times n^{\\ell - 1}} \n",
    "\\underbrace{\\begin{bmatrix} a^{\\ell - 1}_{1,1} & a^{\\ell - 1}_{1, 2} & a^{\\ell - 1}_{1, 3} & \\dots & a^{\\ell - 1}_{1, m} \\\\ \n",
    "\\\\\n",
    "a^{\\ell - 1}_{2,1} & a^{\\ell - 1}_{2, 2} & a^{\\ell - 1}_{2, 3} & \\dots & a^{\\ell - 1}_{2, m} \\\\ \n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n",
    "\\\\\n",
    "a^{\\ell - 1}_{n^{\\ell - 1},1} & a^{\\ell - 1}_{n^{\\ell - 1}, 2} & a^{\\ell - 1}_{n^{\\ell - 1}, 3} & \\dots & a^{\\ell - 1}_{n^{\\ell - 1}, m} \\end{bmatrix}}_{n^{\\ell - 1}  \\times m} + \n",
    "\\underbrace{\\begin{bmatrix} b^{\\ell}_{1,1} & b^{\\ell}_{1, 2} & b^{\\ell}_{1, 3} & \\dots & b^{\\ell}_{1, m} \\\\ \n",
    "\\\\\n",
    "b^{\\ell}_{2,1} & b^{\\ell}_{2, 2} & b^{\\ell}_{2, 3} & \\dots & b^{\\ell}_{2, m} \\\\ \n",
    "\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots &  \\vdots \\\\\n",
    "\\\\\n",
    "b^{\\ell}_{n^{\\ell},1} & b^{\\ell}_{n^{\\ell}, 2} & b^{\\ell}_{n^{\\ell}, 3} & \\dots & b^{\\ell}_{n^{\\ell}, m} \\end{bmatrix}}_{n^{\\ell}  \\times m}\n",
    "\\end{align*}\n",
    "\n",
    "Finally, we can again apply an activation function $g$ to the matrix $Z^{\\ell}$ element-wise to obtain the final activation matrix $A^{\\ell}$ for all $m$ training examples. The columns of the matrix $A^{\\ell}$ are therefore as follows:\n",
    "\n",
    "* the first column represents the activation vector outputted by layer $\\ell$ with $n^{\\ell}$ neurons or units for the **first** training example \n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a^{\\ell}_{1, 1} \\\\\n",
    "\\\\\n",
    "a^{\\ell}_{2, 1} \\\\\n",
    "\\\\\n",
    "\\vdots \\\\\n",
    "\\\\\n",
    "a^{\\ell}_{n^{\\ell}, 1} \\end{bmatrix}}_{n^{\\ell} \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "* the second column represents the activation vector outputted by layer $\\ell$ with $n^{\\ell}$ neurons or units for the **second** training example \n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a^{\\ell}_{1, 2} \\\\\n",
    "\\\\\n",
    "a^{\\ell}_{2, 2} \\\\\n",
    "\\\\\n",
    "\\vdots \\\\\n",
    "\\\\\n",
    "a^{\\ell}_{n^{\\ell}, 2} \\end{bmatrix}}_{n^{\\ell} \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "$\\hspace{11cm} \\vdots$\n",
    "\n",
    "* the $\\mathbf{m}^{\\mathbf{th}}$ column represents the activation vector outputted by layer $\\ell$ with $n^{\\ell}$ neurons or units for the $\\mathbf{m}^{\\mathbf{th}}$ training example \n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{\\begin{bmatrix}\n",
    "a^{\\ell}_{1, m} \\\\\n",
    "\\\\\n",
    "a^{\\ell}_{2, m} \\\\\n",
    "\\\\\n",
    "\\vdots \\\\\n",
    "\\\\\n",
    "a^{\\ell}_{n^{\\ell}, m} \\end{bmatrix}}_{n^{\\ell} \\times 1}\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "**Note** that this is not the only model representation. Some textbooks and tutorials present the weight matrix as follows:\n",
    "\n",
    "\\begin{align*}\n",
    "\\underbrace{W^{\\ell}}_{n^{\\ell - 1} \\times n^{\\ell}}\n",
    "\\end{align*}\n",
    "\n",
    "This is the transpose of our representation at the very top. But the two model representations are equivalent as long as we multiple the relevant matrices correctly. Had we used this representation of the weight matrix, then the the other matrices $A^{\\ell}$, $A^{0}=X$, and $B^{\\ell}$ would have had different dimensions as well. One key difference to note is as follows:\n",
    "\n",
    ">In this representation, each column of the weight matrix $\\underbrace{W^{\\ell}}_{n^{\\ell - 1} \\times n^{\\ell}}$ represents a neuron unit of the layer; in our representation of the mode, the rows of the matrix $\\underbrace{W^{\\ell}}_{n^{\\ell} \\times n^{\\ell - 1}}$ represent the neuron units of the layer. \n",
    "\n",
    "This second representation of the weight matrix also matches how we implemented the vectorized version of neural network in the `neural_network_vectorized.ipynb` notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow Model Representation\n",
    "\n",
    "The TensorFlow model representation of the weight matrix $W$ is as follows:\n",
    "\n",
    "* The weight matrix $W^{\\ell}$ we defined above has dimensions $n^{\\ell} \\times n^{\\ell - 1}$\n",
    "\n",
    "* The weight matrix for layer $\\ell$ in TensorFlow has dimensions $n^{\\ell - 1} \\times n^{\\ell}$\n",
    "\n",
    "The bias vector $\\vec{b}^{\\ell}$ remains the same with dimensions $n^{\\ell} \\times 1$, but it is represented as a 1-d array with shape $(n^{\\ell}, )$ than a 2-d array with shape $(n^{\\ell}, 1)$. Suppose we have the following neural network:\n",
    "\n",
    "<center> <img src=\"images/C2_W1_Assign1.PNG\" width=\"450\" height=\"350\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential(\n",
    "    [               \n",
    "        Input(shape=(400,)),    # Specify input size (400,) or (400,1) column vector for each of the m training examples\n",
    "        Dense(units=25, activation=\"relu\", use_bias=True, name='layer_1'), # 25 neurons in layer 1 so n^(1) = 25\n",
    "        Dense(units=15, activation=\"relu\", use_bias=True, name='layer_2'), # 15 neurons in layer 2 so n^(2) = 15\n",
    "        Dense(units=1, activation=\"sigmoid\", use_bias=True, name='output_layer') # 1 neuron in output layer so n^(3) = 1\n",
    "    ], name = \"my_model\" \n",
    ")     "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we have instantiated an input `keras.engine.keras_tensor.KerasTensor`, we can view the shapes of the weight matrices and bias vectors for each layer in our network. Note that we have not trained the model yet, so the values of the weights are randomly initialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " layer_1 (Dense)             (None, 25)                10025     \n",
      "                                                                 \n",
      " layer_2 (Dense)             (None, 15)                390       \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 1)                 16        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,431\n",
      "Trainable params: 10,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight matrices and bias vectors for these three layers have the following shapes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W^1 shape = (400, 25), b^1 shape = (25,)\n",
      "W^2 shape = (25, 15), b^2 shape = (15,)\n",
      "W^3 shape = (15, 1), b^3 shape = (1,)\n"
     ]
    }
   ],
   "source": [
    "[layer1, layer2, layer3] = model.layers\n",
    "\n",
    "W1, b1 = layer1.get_weights()\n",
    "W2, b2 = layer2.get_weights()\n",
    "W3, b3 = layer3.get_weights()\n",
    "print(f\"W^1 shape = {W1.shape}, b^1 shape = {b1.shape}\")\n",
    "print(f\"W^2 shape = {W2.shape}, b^2 shape = {b2.shape}\")\n",
    "print(f\"W^3 shape = {W3.shape}, b^3 shape = {b3.shape}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes are as expected:\n",
    "\n",
    "* The weight matrix $W^{1}$ in TensorFlow has shape $n^{0} \\times n^{1} = 400 \\times 25$ and $\\vec{b}^{1}$ has shape $(n^{1},)=(25,)$\n",
    "\n",
    "* The weight matrix $W^{2}$ in TensorFlow has shape $n^{1} \\times n^{2} = 25 \\times 15$ and $\\vec{b}^{2}$ has shape $(n^{2},)=(15,)$\n",
    "\n",
    "* The weight matrix $W^{3}$ in TensorFlow has shape $n^{2} \\times n^{3} = 15 \\times 1$ and $\\vec{b}^{3}$ has shape $(n^{3},)=(1,)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
