{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Softmax Function\n",
    "\n",
    "The softmax function (also known as softargmax or normalized exponential function) converts a vector of $K$ real numbers into a probability distribution of $K$ possible outcomes. It is a generalization of the logistic function to multiple dimensions, and used in multinomial logistic regression (Softmax Regression) and in Neural Networks when solving Multiclass Classification problems.\n",
    "\n",
    "<center>  <img  src=\"images/C2_W2_Softmax_Header.PNG\" width=\"1100\" />  </center>\n",
    "\n",
    "Each of the $j \\in 1 ,..., N$ linear predictors\n",
    "\n",
    "$$z_{j}=\\vec{w}_{j}\\vec{x}+b_{j}$$\n",
    "\n",
    "is a binary classification problem where the linear predictor $z_{j}$ is mapped to a probability via the softmax function\n",
    "\n",
    "$$a_j=\\frac{e^{z_j}}{\\sum_{k=1}^{N} e^{z_k}}=\\mathrm{P}(\\mathrm{y}=j \\mid \\overrightarrow{\\mathrm{x}})$$\n",
    "\n",
    "and $\\mathrm{P}(\\mathrm{y}=j \\mid \\overrightarrow{\\mathrm{x}})$ indicates the probability that the output is class $j$.\n",
    "\n",
    "\n",
    "**Note**: We start counts with 0 and ending with N-1,  $\\sum_{i=0}^{N-1}$ and not 1 and end with N,  $\\sum_{i=1}^{N}$. This is because computer programs will typically start iterating with 0 but mathematically we usually index from 1 to N."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('utils/deeplearning.mplstyle')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "from matplotlib.widgets import Slider\n",
    "from utils.lab_utils_common import dlc\n",
    "from utils.lab_utils_softmax import plt_softmax\n",
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "tf.autograph.set_verbosity(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Softmax Function\n",
    "In both softmax regression and neural networks with Softmax outputs, N outputs are generated and one output is selected as the predicted category. In both cases a vector $\\mathbf{z}$ is generated by a linear function which is applied to a softmax function. The softmax function converts $\\mathbf{z}$  into a probability distribution as described below. After applying softmax, each output will be between 0 and 1 and the outputs will add to 1, so that they can be interpreted as probabilities. The larger inputs  will correspond to larger output probabilities.\n",
    "\n",
    "<center>  <img  src=\"images/C2_W2_SoftmaxReg_NN.png\" width=\"800\" />  </center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The softmax function can be written:\n",
    "\\begin{align*}\n",
    "a_j = \\frac{e^{z_j}}{ \\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\end{align*}\n",
    "\n",
    "The output $\\vec{a}$ is a vector of length N, so for softmax regression, we could also write:\n",
    "\\begin{align*}\n",
    "\\vec{a}(x) =\n",
    "\\begin{bmatrix}\n",
    "P(y = 1 | \\mathbf{x}; \\mathbf{w},b) \\\\\n",
    "\\vdots \\\\\n",
    "P(y = N | \\mathbf{x}; \\mathbf{w},b)\n",
    "\\end{bmatrix}=\\frac{1}{\\sum_{k=1}^{N}{e^{z_k} }}\n",
    "\\begin{bmatrix}\n",
    "e^{z_1} \\\\\n",
    "\\vdots \\\\\n",
    "e^{z_{N}} \\\\\n",
    "\\end{bmatrix}\n",
    "\\end{align*}\n",
    "\n",
    "This shows that the output is a **vector of probabilities**. The first entry is the probability the output is the first category given the input $\\mathbf{x}$ and parameters $\\mathbf{w}$ and $\\mathbf{b}$. Let's create a NumPy implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_softmax(z: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Softmax function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    z : np.ndarray\n",
    "        A vector of linear predictors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A vector of probabilities.\n",
    "    \"\"\"\n",
    "    ez = np.exp(z)              # Element-wise exponentiation\n",
    "    sm = ez / np.sum(ez)\n",
    "    return sm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, vary the values of the `z` inputs using the sliders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.close(\"all\")\n",
    "plt_softmax(my_softmax)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we vary the values of the linear predictors z's above, there are a few things to note:\n",
    "* the exponential in the numerator of the softmax magnifies small differences in the values \n",
    "* the output values sum to one\n",
    "* the softmax spans all of the outputs. A change in `z0` for example will change the values of `a0`-`a3`. Compare this to other activations such as ReLU or Sigmoid which have a single input and single output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Cost\n",
    "<center> <img  src=\"images/C2_W2_SoftMaxCost.png\" width=\"700\" />    </center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss function associated with Softmax, the cross-entropy loss, is:\n",
    "\\begin{equation}\n",
    "  L(\\vec{a},y)=\\begin{cases}\n",
    "    -log(a_1), & \\text{if $y=1$}.\\\\\n",
    "        &\\vdots\\\\\n",
    "     -log(a_N), & \\text{if $y=N$}\n",
    "  \\end{cases} \\tag{3}\n",
    "\\end{equation}\n",
    "\n",
    "Where y is the target category for this example (a row in the data matrix) and $\\vec{a}$ is the output of a softmax function. In particular, the values in $\\vec{a}$ are probabilities that sum to one.\n",
    "\n",
    ">**Recall:** Loss is defined for one training example (a row or one observation) while Cost covers all training examples. \n",
    " \n",
    " \n",
    "Note in (3) above, only the line that corresponds to the target contributes to the loss, other lines are zero. To write the cost equation we need an 'indicator function' that will be 1 when the index matches the target and zero otherwise. \n",
    "    $$\\mathbf{1}\\{y == n\\} = =\\begin{cases}\n",
    "    1, & \\text{if $y==n$}.\\\\\n",
    "    0, & \\text{otherwise}.\n",
    "  \\end{cases}$$\n",
    "Now the cost is:\n",
    "\\begin{align}\n",
    "J(\\mathbf{w},b) = -\\frac{1}{m} \\left[ \\sum_{i=1}^{m} \\sum_{j=1}^{N}  1\\left\\{y^{(i)} == j\\right\\} \\log \\frac{e^{z^{(i)}_j}}{\\sum_{k=1}^N e^{z^{(i)}_k} }\\right] \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "Where $m$ is the number of examples, $N$ is the number of outputs. This is the average of all the losses.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow\n",
    "\n",
    "There are two ways of implementing the softmax using the cross-entropy loss in Tensorflow; the first is the most straightforward while the second is more numerically stable. Let's start by creating a dataset to train a multiclass classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make  dataset for example\n",
    "centers = [[-5, 2], [-2, -2], [1, 2], [5, -2]]\n",
    "X_train, y_train = make_blobs(n_samples=2000, centers=centers, cluster_std=1.0,random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 2), (2000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data set contains two features (columns). It has the following four classes in its labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Straightforward Organization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model below is implemented with the softmax as an activation function in the final output or dense layer. The loss function is separately specified in the `compile` directive. \n",
    "\n",
    "The loss function is `SparseCategoricalCrossentropy`, which is described in (3) above. In this model, the softmax takes place in the last layer. The loss function takes in the softmax output, which is a vector of probabilities $\\vec{a}=\\begin{bmatrix}a_{1} \\\\ \\vdots \\\\ a_{N}\\end{bmatrix}$. Note that the hidden layers use `relu` as activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 1s 2ms/step - loss: 0.9594\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.3464\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1586\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.1005\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0765\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0636\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0556\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0502\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0459\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.0427\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x146b04d90>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'softmax')    # < Softmax activation\n",
    "    ]\n",
    ")\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=10 # The entire dataset is passed forward in the network 10 times\n",
    ")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the softmax is integrated into the output layer, the output is a vector of probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 778us/step\n",
      "The shape of the output is (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "p_nonpreferred = model.predict(X_train)\n",
    "print('The shape of the output is', p_nonpreferred.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each of the 2000 training example (sample or row), there are four probabilities corresponding to the four classes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.66e-03, 2.07e-03, 9.75e-01, 1.68e-02],\n",
       "       [9.90e-01, 1.03e-02, 4.77e-05, 6.81e-05],\n",
       "       [9.48e-01, 5.07e-02, 5.23e-04, 6.60e-04],\n",
       "       ...,\n",
       "       [2.33e-03, 9.92e-01, 1.71e-03, 3.69e-03],\n",
       "       [9.80e-05, 1.04e-04, 5.39e-04, 9.99e-01],\n",
       "       [9.06e-03, 1.76e-03, 9.85e-01, 4.04e-03]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_nonpreferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest value 0.99999636 smallest value 1.561581e-08\n"
     ]
    }
   ],
   "source": [
    "print(\"largest value\", np.max(p_nonpreferred), \"smallest value\", np.min(p_nonpreferred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preferred Organization\n",
    "\n",
    "\n",
    "<center> <img  src=\"images/C2_W2_softmax_accurate.png\" width=\"700\" />    </center>\n",
    "\n",
    "The more stable and accurate results can be obtained if the **softmax and loss are combined during training**. This is enabled by the 'preferred' organization shown below.\n",
    "\n",
    "In the preferred organization, the final layer has a linear activation (or no activation). For historical reasons, the outputs in this form are referred to as *logits*. The loss function has an additional argument: `from_logits = True`. This informs the loss function that the softmax operation should be included in the loss calculation as supposed to using the probabilities values in the vector $\\vec{a}$. This allows for an optimized implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "63/63 [==============================] - 1s 1ms/step - loss: 1.0442\n",
      "Epoch 2/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.4408\n",
      "Epoch 3/10\n",
      "63/63 [==============================] - 0s 1ms/step - loss: 0.2062\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.1173\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 0s 3ms/step - loss: 0.0831\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0666\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0572\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0510\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0463\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 0s 2ms/step - loss: 0.0426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1472caee0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preferred_model = Sequential(\n",
    "    [ \n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(4, activation = 'linear')   # Use linear activation\n",
    "    ]\n",
    ")\n",
    "preferred_model.compile(\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),  # Set from_logits=True\n",
    "    optimizer=tf.keras.optimizers.Adam(0.001),\n",
    ")\n",
    "\n",
    "preferred_model.fit(\n",
    "    X_train,y_train,\n",
    "    epochs=10\n",
    ")\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Handling\n",
    "Notice that in the preferred model, the outputs are not probabilities, but can range from large negative numbers to large positive numbers. The output must be sent through a softmax when performing a prediction that expects a probability. \n",
    "Let's look at the preferred model outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 0s 993us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-3.01, -2.69,  1.65, -1.84],\n",
       "       [ 5.84, -0.68, -4.69, -8.77],\n",
       "       [ 3.98, -0.26, -3.51, -6.87],\n",
       "       ...,\n",
       "       [-4.9 ,  3.24, -2.39, -1.95],\n",
       "       [-4.98, -2.47, -4.67,  4.34],\n",
       "       [-2.49, -2.23,  2.68, -2.89]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_preferred = preferred_model.predict(X_train)\n",
    "p_preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest value 9.3184185 smallest value -17.259165\n"
     ]
    }
   ],
   "source": [
    "print(\"largest value\", np.max(p_preferred), \"smallest value\", np.min(p_preferred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output predictions are not probabilities!\n",
    "If the desired output are probabilities, then the output should be be processed by a [softmax](https://www.tensorflow.org/api_docs/python/tf/nn/softmax)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.99e-03, 1.25e-02, 9.49e-01, 2.91e-02],\n",
       "       [9.98e-01, 1.47e-03, 2.65e-05, 4.50e-07],\n",
       "       [9.85e-01, 1.42e-02, 5.51e-04, 1.90e-05],\n",
       "       ...,\n",
       "       [2.87e-04, 9.91e-01, 3.53e-03, 5.53e-03],\n",
       "       [8.96e-05, 1.10e-03, 1.22e-04, 9.99e-01],\n",
       "       [5.63e-03, 7.28e-03, 9.83e-01, 3.77e-03]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_preferred = tf.nn.softmax(p_preferred).numpy()\n",
    "sm_preferred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "largest value 0.9999966 smallest value 9.179213e-12\n"
     ]
    }
   ],
   "source": [
    "print(\"largest value\", np.max(sm_preferred), \"smallest value\", np.min(sm_preferred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the most likely category, the softmax is not required. One can find the index of the largest output using [np.argmax()](https://numpy.org/doc/stable/reference/generated/numpy.argmax.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.01 -2.69  1.65 -1.84], The NN predicts category: 2\n",
      "[ 5.84 -0.68 -4.69 -8.77], The NN predicts category: 0\n",
      "[ 3.98 -0.26 -3.51 -6.87], The NN predicts category: 0\n",
      "[-4.12  3.53 -1.86 -2.55], The NN predicts category: 1\n",
      "[-2.05 -3.41  3.59 -6.18], The NN predicts category: 2\n",
      "[-3.5  -3.2   2.77 -3.05], The NN predicts category: 2\n",
      "[-5.47  3.74 -2.71 -2.22], The NN predicts category: 1\n",
      "[  7.02  -1.87  -4.94 -11.49], The NN predicts category: 0\n",
      "[-4.53  4.   -2.09 -2.85], The NN predicts category: 1\n",
      "[-4.48 -2.53 -3.35  3.15], The NN predicts category: 3\n",
      "[-2.82  1.5  -1.19 -1.06], The NN predicts category: 1\n",
      "[-4.34 -4.    3.45 -3.88], The NN predicts category: 2\n",
      "[  5.09  -3.06  -2.01 -12.16], The NN predicts category: 0\n",
      "[-4.7  -4.6   4.75 -5.4 ], The NN predicts category: 2\n",
      "[-3.63 -3.3   2.19 -2.48], The NN predicts category: 2\n",
      "[ 6.48 -1.17 -4.91 -9.69], The NN predicts category: 0\n",
      "[-7.75  6.21 -3.93 -3.86], The NN predicts category: 1\n",
      "[-2.19 -1.94  1.74 -1.78], The NN predicts category: 2\n",
      "[-4.33 -1.39 -4.04  3.57], The NN predicts category: 3\n",
      "[  7.64  -2.12  -5.3  -12.61], The NN predicts category: 0\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    # Note that p_preferred[i] is a 1D array of length 4 (4,) so argmax finds the index of the largest value over the 1st axis 0\n",
    "    print( f\"{p_preferred[i]}, The NN predicts category: {np.argmax(p_preferred[i], axis=0)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparseCategorialCrossentropy or CategoricalCrossEntropy\n",
    "\n",
    "Tensorflow has two potential formats for target values and the selection of the loss defines.\n",
    "\n",
    "- `SparseCategorialCrossentropy` expects the target to be an integer corresponding to the index. For example, if there are 10 potential target values, y would be between 0 and 9. \n",
    "  \n",
    "- `CategoricalCrossEntropy` expects the target value of an example to be one-hot encoded where the value at the target index is 1 while the other N-1 entries are zero. Each example or row of the data matrix has 10 potential target values (columns). For instance, if the $ith$ training example has target label class equaling 2, then `X_train[i]` would be [0,0,1,0,0,0,0,0,0,0].\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_for_machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
