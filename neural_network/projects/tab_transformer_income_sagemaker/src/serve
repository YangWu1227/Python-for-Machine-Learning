#!/usr/bin/env python3

import ast
import json
import logging
import os
import sys
import time
import traceback
from functools import lru_cache
from typing import Any, Dict, List, Tuple, Union

import flask
import polars as pl
import tensorflow as tf
from flask import Flask, Response


def serve() -> None:
    """
    Flask Inference Server for SageMaker hosting of Keras Model.
    """
    app = Flask(__name__)
    logging.basicConfig(level=logging.DEBUG)

    app.logger.info(f'{os.cpu_count()} CPUs detected')

    @app.route('/ping', methods=['GET'])
    def ping() -> Response:
        """
        SageMaker required method, ping heartbeat.

        Returns
        -------
        Response
            HTTP response with status 200.
        """
        return Response(response='\n', status=200)

    @lru_cache()
    def load_trained_model() -> Tuple[tf.keras.Model, str]:
        """
        Cached loading of trained Keras model into memory.

        Returns
        -------
        Tuple[tf.keras.Model, str]
            Tuple containing the reloaded model and its filename.
        """
        model_dir = '/opt/ml/model/00000000'
        app.logger.info(f'Loading model from : {model_dir}')

        start_time = time.perf_counter()

        reloaded_model = tf.keras.models.load_model(model_dir)

        exec_time = time.perf_counter() - start_time
        app.logger.info(f'Model {model_dir} loaded in {exec_time:.5f}s')

        return reloaded_model, model_dir

    @app.route('/invocations', methods=['POST'])
    def predict() -> Response:
        """
        Run CPU inference on input data, called every time an incoming request arrives.

        Returns
        -------
        Response
            HTTP response with status 200 and prediction results.
        """
        try:
            data_list = ast.literal_eval(flask.request.get_data().decode('utf-8'))

            # If the data_list is not a list of lists (i.e., it's a single row), make it a list of lists
            if not isinstance(data_list[0], list):
                data_list = [data_list]

            # Specify the feature names and types
            feature_dtypes = {
                'age': float,
                'workclass': str,
                'education': str,
                'education_num': float,
                'marital_status': str,
                'occupation': str,
                'relationship': str,
                'race': str,
                'gender': str,
                'capital_gain': float,
                'capital_loss': float,
                'hours_per_week': float,
                'native_country': str
            }

            # Convert the list of lists to a numpy array with the specified dtype
            data = pl.DataFrame(data=data_list, schema=feature_dtypes)

            # Convert to tf.data.Dataset
            dataset = tf.data.Dataset.from_tensor_slices(data.to_dict()).batch(256)

            # Add an extra dimension to match the model's expected input shape (each x is a feature_name -> value dict)
            dataset = dataset.map(lambda x: {k: tf.expand_dims(v, 1) for k, v in x.items()})

        except Exception as e:
            return Response(
                response='Unable to parse the input data: ' + str(e),
                status=415,
                mimetype='text/csv',
            )

        # Cached [reloading] of trained model to process incoming requests
        reloaded_model, model_dir = load_trained_model()

        try:
            start_time = time.perf_counter()
            app.logger.info(f'Running inference using the Keras model: {model_dir}')

            # Predictions are a tensor with shape (n_samples, 1), which needed to be converted to an numpy array
            predictions = tf.nn.sigmoid(reloaded_model.predict(dataset)).numpy()

            # Convert numpy array to list of lists and serialize as JSON formatted string
            predictions_str = json.dumps(predictions.tolist())

            exec_time = time.perf_counter() - start_time
            app.logger.info(f'Inference finished in {exec_time:.5f}s')

            # Return predictions as a string
            return Response(
                response=predictions_str, status=200, mimetype='text/csv'
            )

        # Error during inference
        except Exception as inference_error:
            app.logger.error(inference_error)
            return Response(
                response=f'Inference failure: {inference_error}', status=400, mimetype='text/csv'
            )

    # Initial [non-cached] reload of trained model
    reloaded_model, model_dir = load_trained_model()

    # Trigger start of Flask app
    app.run(host='0.0.0.0', port=8080)


if __name__ == '__main__':

    try:
        serve()
        sys.exit(0)  # Success exit code

    except Exception:
        
        traceback.print_exc()
        sys.exit(-1) # Failure exit code