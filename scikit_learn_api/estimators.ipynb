{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Estimators\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "- [Regression](#regression)\n",
    "  - [Linear Regression](#linear-regression)\n",
    "  - [Decision Tree Regressor](#decision-tree-regressor)\n",
    "  - [Voting Regressor](#voting-regressor)\n",
    "  - [Bagging Regressor](#bagging-regressor)\n",
    "- [Classification](#classification)\n",
    "  - [Decision Tree Classifier](#decision-tree-classifier)\n",
    "  - [Bagging Classifier](#bagging-classifier)\n",
    "  - [Voting Classifier](#voting-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LinearRegression(\n",
    "    fit_intercept=True, \n",
    "    normalize='deprecated', \n",
    "    copy_X=True, \n",
    "    n_jobs=None, \n",
    "    positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Requires standardization (subtract mean and divide by standard deviation). Based on Kutner, M. H., Nachtsheim, C., Neter, J. Applied linear statistical models.\n",
    "\n",
    "* Multicollinearity- when predictor variables are correlated, the regression coefficient of any one variable depends on which other predictor variables are included and which ones are left out. The common interpretation of a regression coefficient as measuring the change in the expected value of the response variable when the given predictor is increased by one unit while all other predictor variables are held constant is not fully applicable.\n",
    "\n",
    "* Sensitive to outliers, influential observations:\n",
    "\n",
    "    * Outlying target values (studentized deleted residuals, studentized residuals)\n",
    "  \n",
    "    * Outlying X observations (hat matrix, leverage values)\n",
    "\n",
    "    * Influential cases (DFFITS, Cook's Distance, DFBETAS)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "DecisionTreeRegressor(\n",
    "    criterion='squared_error', \n",
    "    splitter='best', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features=None, \n",
    "    random_state=None, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    ccp_alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* High variance (tend to overfit the training data).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples_leaf (Regularizer)\n",
    "\n",
    "* min_samples_split (Regularizer)\n",
    "\n",
    "* max_depth (Pruning)\n",
    "\n",
    "* max_features (Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'max_features': ['sqrt', 'log2'], 'ccp_alpha': [0.25, 0.5, 0.75], 'min_samples_split': [5, 6, 7, 8, 9, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "BaggingRegressor(\n",
    "    # If None, then the base estimator is a DecisionTreeRegressor\n",
    "    base_estimator=None, \n",
    "    # Number of base estimators in the ensemble\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0, \n",
    "    max_features=1.0,\n",
    "    # Bootstrap samples with replacement\n",
    "    bootstrap=True, \n",
    "    bootstrap_features=False,\n",
    "    # Use out-of-bag samples to estimate the generalization error\n",
    "    oob_score=True, \n",
    "    warm_start=False,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "VotingRegressor(\n",
    "    # List of (str, estimator) tuples\n",
    "    estimators=[('lr', LinearRegression()), ('dt', DecisionTreeRegressor())],\n",
    "    weights=None, \n",
    "    n_jobs=None, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "* Generally taken out towards the end of the project once there are a few strong candidate models. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DecisionTreeRegressor(\n",
    "    criterion='gini', \n",
    "    splitter='best', \n",
    "    # Hyperparameter\n",
    "    max_depth=None, \n",
    "    # Hyperparameter\n",
    "    min_samples_split=2, \n",
    "    # Hyperparameter\n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    # Hyperparameter\n",
    "    max_features=None, \n",
    "    random_state=None, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    class_weight=None, \n",
    "    # Hyperparameter\n",
    "    ccp_alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* High variance (tend to overfit the training data).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples_leaf (Regularizer)\n",
    "\n",
    "* min_samples_split (Regularizer)\n",
    "\n",
    "* max_depth (Pruning)\n",
    "\n",
    "* max_features (Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'max_features': ['sqrt', 'log2'], 'ccp_alpha': [0.25, 0.5, 0.75], 'min_samples_split': [5, 6, 7, 8, 9, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "BaggingClassifier(\n",
    "    # If None, then the base estimator is a DecisionTreeClassifier\n",
    "    base_estimator=None, \n",
    "    # Number of base estimators in the ensemble\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0, \n",
    "    max_features=1.0,\n",
    "    # Bootstrap samples with replacement\n",
    "    bootstrap=True, \n",
    "    bootstrap_features=False,\n",
    "    # Use out-of-bag samples to estimate the generalization error\n",
    "    oob_score=True, \n",
    "    warm_start=False,\n",
    "    random_state=None,\n",
    "    n_jobs=None,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "VotingClassifier(\n",
    "    estimators=[('dt', DecisionTreeClassifier())],\n",
    "    # Hard or soft voting\n",
    "    voting='hard', \n",
    "    weights=None, \n",
    "    n_jobs=None, \n",
    "    # If voting='soft' and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes)\n",
    "    # If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes)\n",
    "    flatten_transform=True, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "* If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the `argmax` of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. All classifier must have a `predict_proba` method."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('python_for_machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
