{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Estimators\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "- [Regression](#regression)\n",
    "  - [Distance Based Regressor](#distance-based-regression)\n",
    "    - [Linear Regression](#linear-regression)\n",
    "    - [IRLS Robust Regression With Huber Weight Function](#irls-robust-regression-with-huber-weight-function)\n",
    "    - [Random Sample Consensus](#random-sample-consensus)\n",
    "  - [Tree Based Regressor](#tree-based-regression)\n",
    "    - [Decision Tree Regressor](#decision-tree-regressor)\n",
    "    - [Voting Regressor](#voting-regressor)\n",
    "    - [Bagging Regressor](#bagging-regressor)\n",
    "    - [Random Forest Regressor](#random-forest-regressor)\n",
    "    - [Extra Tree Regressor](#extra-tree-regressor)\n",
    "    - [AdaBoost Regressor](#adaboost-regressor)\n",
    "- [Classification](#classification)\n",
    "  - [Distance Based Classifier](#distance-based-classifier)\n",
    "    - [Logistic Regression](#logistic-regression)\n",
    "  - [Tree Based Classifier](#tree-based-classifer)\n",
    "    - [Decision Tree Classifier](#decision-tree-classifier)\n",
    "    - [Bagging Classifier](#bagging-classifier)\n",
    "    - [Voting Classifier](#voting-classifier)\n",
    "    - [Random Forest Classifier](#random-forest-classifier)\n",
    "    - [Extra Tree Classifier](#extra-tree-classifier)\n",
    "    - [AdaBoost Classifier](#adaboost-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Based Regressor\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LinearRegression(\n",
    "    fit_intercept=True, normalize=\"deprecated\", copy_X=True, n_jobs=-1, positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Requires standardization (subtract mean and divide by standard deviation). Based on Kutner, M. H., Nachtsheim, C., Neter, J. Applied linear statistical models.\n",
    "\n",
    "* Multicollinearity- when predictor variables are correlated, the regression coefficient of any one variable depends on which other predictor variables are included and which ones are left out. The common interpretation of a regression coefficient as measuring the change in the expected value of the response variable when the given predictor is increased by one unit while all other predictor variables are held constant is not fully applicable.\n",
    "\n",
    "* Sensitive to outliers, influential observations:\n",
    "\n",
    "    * Outlying target values (studentized deleted residuals, studentized residuals)\n",
    "  \n",
    "    * Outlying X observations (hat matrix, leverage values)\n",
    "\n",
    "    * Influential cases (DFFITS, Cook's Distance, DFBETAS)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IRLS Robust Regression With Huber Weight Function\n",
    "\n",
    "For robust regression, weighted least squares is used to reduce the influence of outlying cases by employing weights that vary inversely with the size of the residual. The weight function for each training sample has the following formulation based on the Applied Linear Regression Models book:\n",
    "\n",
    "$$\n",
    "w= \\begin{cases}1 & |u| \\leq 1.345 \\\\ \\frac{1.345}{|u|} & |u|>1.345\\end{cases}\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "* $u_{i}=\\frac{Y_{i} - \\hat{Y_{i}}}{M A D}=\\frac{e_{i}}{M A D}$\n",
    "  \n",
    "* $M A D=\\frac{1}{.6745} \\text { median }\\left\\{\\left|e_{i}-\\operatorname{median}\\left\\{e_{i}\\right\\}\\right|\\right\\}$ and the constant $.6745$ provides an approximately unbiased estimate of $\\sigma$ for independent observations from a normal distribution.\n",
    "\n",
    "The scikit-learn formation states that the `HuberRegressor` optimizes the squared loss for the samples where `|(y - X'w) / sigma| < epsilon` and the absolute loss for the samples where `|(y - X'w) / sigma| > epsilon`, where `w` and `sigma` are parameters to be optimized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import HuberRegressor\n",
    "\n",
    "HuberRegressor(\n",
    "    epsilon=1.35,\n",
    "    # This needs to be tweaked to get convergence\n",
    "    max_iter=1000,\n",
    "    alpha=0.0001,\n",
    "    warm_start=False,\n",
    "    fit_intercept=True,\n",
    "    tol=1e-05,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Requires standardization (subtract mean and divide by standard deviation).\n",
    "\n",
    "* Multicollinearity- when predictor variables are correlated, the regression coefficient of any one variable depends on which other predictor variables are included and which ones are left out. The common interpretation of a regression coefficient as measuring the change in the expected value of the response variable when the given predictor is increased by one unit while all other predictor variables are held constant is not fully applicable.\n",
    "\n",
    "* The loss function is not heavily influenced by the outliers while not completely ignoring their effect. May need to conduct diagnostics on univariate predictors. \n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* alpha\n",
    "\n",
    "* warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import reciprocal\n",
    "\n",
    "# Distribution for 'alpha'\n",
    "reciprocal_distrib = reciprocal(0.0001, 0.01)\n",
    "# Generate a random numbers\n",
    "samples = reciprocal_distrib.rvs(10000, random_state=12)\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.title(\"Reciprocal distribution (scale=1.0)\")\n",
    "plt.hist(samples, bins=50)\n",
    "plt.show()\n",
    "\n",
    "# Parameter space with RandomizedSearchCV\n",
    "parameter = {\"alpha\": reciprocal(0.0001, 0.01), \"warm_start\": [True, False]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Sample Consensus\n",
    "\n",
    "The algorithm splits the complete input sample data into a set of inliers, which may be subject to noise and outliers, which could be caused by erroneous measurements or invalid hypotheses about the data. The resulting model is then estimated **only** from the determined inliers. The `RANSAC` is a non-deterministic algorithm producing only a reasonable result with a certain probability, which is dependent on the number of iterations specified by the `max_trials` parameter. Assuming that the $n$ points needed for estimating a model are selected independently dently, $w^{n}$ is the probability that all n points are inliers and $1-w^{n}$ is the probability that at least one of the n points is an outlier, a case which implies that a bad model will be estimated from this point set. That probability to the power of $k$ is the probability that the algorithm never selects a set of $n$ points which all are inliers and this must be the same as $1-p$. Consequently,\n",
    "$$\n",
    "1-p=\\left(1-w^{n}\\right)^{k}\n",
    "$$\n",
    "which, after taking the logarithm of both sides, leads to\n",
    "$$\n",
    "k=\\frac{\\log (1-p)}{\\log \\left(1-w^{n}\\right)}\n",
    "$$\n",
    "\n",
    "Each iteration performs the following steps:\n",
    "\n",
    "1. Select `min_samples` random samples from the original data and check whether the set of data is valid (using `is_data_valid`).\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# This can be any arbitrary function\n",
    "# For instance, it could check that a certain feature contains all classes\n",
    "def is_data_valid(X, y):\n",
    "    assert X.shape[0] == min_samples\n",
    "    assert y.shape[0] == min_samples\n",
    "    return False\n",
    "```\n",
    "<br>\n",
    "\n",
    "2. Fit a model to the random subset (`base_estimator.fit`) and check whether the estimated model is valid (using `is_model_valid`).\n",
    "\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# This can be any arbitrary function\n",
    "    def is_model_valid(estimator, X, y):\n",
    "        assert estimator == something\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "3. Classify all data as **inliers** or **outliers** by calculating the residuals to the estimated model (`base_estimator.predict(X) - y`). All data samples with absolute residuals smaller than or equal to the `residual_threshold` are considered as inliers. By default the threshold is chosen as the `MAD` (median absolute deviation) of the target values `y`.\n",
    "\n",
    "4. Save fitted model as best model if number of inlier samples is maximal. In case the current estimated model has the same number of inliers, it is only considered as the best model if it has better score.\n",
    "\n",
    "These steps are performed either a maximum number of times (`max_trials`) or until one of the special stop criteria are met (`stop_n_inliers` and `stop_score`). The final model is estimated using all inlier samples (consensus set) of the previously determined best model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "from math import inf\n",
    "\n",
    "RANSACRegressor(\n",
    "    estimator=None,\n",
    "    # Need to set this to a reasonable value\n",
    "    min_samples=None,\n",
    "    residual_threshold=None,\n",
    "    is_data_valid=None,\n",
    "    is_model_valid=None,\n",
    "    max_trials=100,\n",
    "    max_skips=inf,\n",
    "    stop_n_inliers=inf,\n",
    "    stop_score=inf,\n",
    "    stop_probability=0.99,\n",
    "    loss=\"absolute_error\",\n",
    "    random_state=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Requires standardization (subtract mean and divide by standard deviation).\n",
    "\n",
    "* Need a `is_data_valid` function.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples\n",
    "\n",
    "* max_trials\n",
    "\n",
    "* loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.zeros((100, 2))\n",
    "# Number of training samples\n",
    "n_train = X.shape[0]\n",
    "\n",
    "parameter = [\n",
    "    {\n",
    "        \"min_samples\": [n_train * 0.25, n_train * 0.5, n_train * 0.75],\n",
    "        \"max_trials\": [100, 200, 500],\n",
    "        \"loss\": [\"absolute_error\", \"squared_error\"],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "DecisionTreeRegressor(\n",
    "    criterion=\"squared_error\",\n",
    "    splitter=\"best\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    # A good value is m = sqrt(p) based on ISL book\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    # Greater values of ccp_alpha increase the number of nodes pruned\n",
    "    ccp_alpha=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* High variance (tend to overfit the training data).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples_leaf\n",
    "\n",
    "* min_samples_split\n",
    "\n",
    "* max_depth \n",
    "\n",
    "* max_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"ccp_alpha\": [0.0, 0.25, 0.5, 0.75],\n",
    "        \"min_samples_split\": [10, 100, 200],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "BaggingRegressor(\n",
    "    # If None, then the base estimator is a DecisionTreeRegressor\n",
    "    base_estimator=None,\n",
    "    # Number of base estimators in the ensemble\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0,\n",
    "    max_features=1.0,\n",
    "    # Bootstrap samples with replacement\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    # Use out-of-bag samples to estimate the generalization error\n",
    "    oob_score=True,\n",
    "    warm_start=False,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "VotingRegressor(\n",
    "    # List of (str, estimator) tuples\n",
    "    estimators=[(\"lr\", LinearRegression()), (\"dt\", DecisionTreeRegressor())],\n",
    "    weights=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "* Generally taken out towards the end of the project once there are a few strong candidate models. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    criterion=\"squared_error\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=1.0,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,\n",
    "    # Set to False by default\n",
    "    oob_score=True,\n",
    "    # Change to -1 to use all cores\n",
    "    n_jobs=-1,\n",
    "    # Controls bootstrapping of the samples when creating trees and getting a random subsets of features to search for the best feature for splitting\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    ")\n",
    "\n",
    "# Roughly equivalent to the following\n",
    "BaggingRegressor(\n",
    "    DecisionTreeRegressor(criterion=\"mse\", splitter=\"random\", max_features=1.0),\n",
    "    n_estimators=100,\n",
    "    random_state=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners. Trading higher bias (underfitting) for a lower variance (overfitting).\n",
    "\n",
    "* Differs from bagging by introducing more randomness and diversity such that each split only considers a subset of the predictors or features, which can be controlled via `max_features`. Instead of searching for the very best feature among all features when splitting a node, it searches for the best feature among a random subset of features.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* `RandomForestRegressor` has most of the hyperparameters of a `DecisionTreeRegressor` (to control how trees are grown) and all the hyperparameters of a `BaggingRegressor` to control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"ccp_alpha\": [0.0, 0.25, 0.5, 0.75],\n",
    "        \"min_samples_split\": [10, 100, 200],\n",
    "        \"max_samples\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "ExtraTreesRegressor(\n",
    "    n_estimators=100,\n",
    "    criterion=\"squared_error\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=1.0,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    # Changed default values from False to True\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Even more randomness and diversity than `RandomForestClassifier` by using random thresholds for each feature rather than searching for the best possible thresholds at each split.\n",
    "\n",
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"ccp_alpha\": [0.0, 0.25, 0.5, 0.75],\n",
    "        \"min_samples_split\": [10, 100, 200],\n",
    "        \"max_samples\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "AdaBoostRegressor(\n",
    "    # If None, then a DecisionTreeRegressor initialized with max_depth=3\n",
    "    base_estimator=None,\n",
    "    # In case of perfect fit, the learning procedure is stopped early\n",
    "    n_estimators=500,\n",
    "    # Higher learng rate penalizes weak learners with higher error rates more and rewards stronger learners with lower error rates more\n",
    "    learning_rate=1.0,\n",
    "    loss=\"linear\",\n",
    "    random_state=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Depending on the base estimator used, inputs may need to be standardized.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* base_estimator\n",
    "\n",
    "* n_estimators (controlling underfitting (high bias) or overderfitting (high variance) trade-off)\n",
    "\n",
    "* learning_rate (how much should worst weak learners be penalized and stronger learners to be rewarded)\n",
    "\n",
    "* loss (the loss function to use when updating the weights after each boosting iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "parameter = [\n",
    "    {\n",
    "        \"base_estimator\": [SVR(kernel=\"rbf\"), LinearSVR()],\n",
    "        \"n_estimators\": [500, 1000],\n",
    "        \"loss\": [\"linear\", \"square\", \"exponential\"],\n",
    "        # Values that Friedman (2001) used in his simulation study\n",
    "        \"learning_rate\": [1.0, 0.5, 0.25, 0.125, 0.06],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "## Distance Based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "LogisticRegression(\n",
    "    # Specify the norm of the penalty, since regularization is applied by default\n",
    "    penalty=\"l2\",\n",
    "    # Prefer dual=False when n_samples (rows) > n_features (columns), which is usually the case\n",
    "    dual=False,\n",
    "    tol=0.0001,\n",
    "    # Must be a positive float where smaller values specify stronger regularization\n",
    "    C=1.0,\n",
    "    fit_intercept=True,\n",
    "    intercept_scaling=1,\n",
    "    # Weights associated with classes in the form {class_label: weight}. If not given, all classes are supposed to have weight one.\n",
    "    class_weight=None,\n",
    "    # Used when solver == ‘sag’, ‘saga’ or ‘liblinear’ to shuffle the data\n",
    "    random_state=None,\n",
    "    # {‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’} default=’lbfgs’\n",
    "    solver=\"lbfgs\",\n",
    "    max_iter=100,\n",
    "    multi_class=\"auto\",\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    n_jobs=None,\n",
    "    l1_ratio=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solver Choice\n",
    "\n",
    "The choice of the algorithm depends on the penalty chosen. Supported penalties by solver:\n",
    "\n",
    "* 'newton-cg' - [‘l2’, ‘none’]\n",
    "\n",
    "* 'lbfgs' - [‘l2’, ‘none’]\n",
    "\n",
    "* ‘liblinear’ - [‘l1’, ‘l2’]\n",
    "\n",
    "* ‘sag’ - [‘l2’, ‘none’]\n",
    "\n",
    "* ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, ‘none’]\n",
    "\n",
    "The ‘sag’ and ‘saga’ fast convergence is only guaranteed on features with approximately the same scale.\n",
    "\n",
    "### Preprocessing\n",
    "\n",
    "* Requires feature scaling.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "The logistic regression does not really have any critical hyperparameters to tune.\n",
    "\n",
    "* solver in [‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’]\n",
    "  \n",
    "Regularization (penalty) can sometimes be helpful.\n",
    "\n",
    "* penalty in [‘none’, ‘l1’, ‘l2’, ‘elasticnet’]\n",
    "\n",
    "The C parameter controls the penalty strength, which can also be effective.\n",
    "\n",
    "* C in [100, 10, 1.0, 0.1, 0.01]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3 x 1 x 4 combinations\n",
    "parameter = [\n",
    "    {\n",
    "        \"solver\": [\"newton-cg\", \"lbfgs\", \"liblinear\"],\n",
    "        \"penalty\": [\"l2\"],\n",
    "        \"min_samples_split\": [100, 10, 1.0, 0.1],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DecisionTreeClassifier(\n",
    "    criterion=\"gini\",\n",
    "    splitter=\"best\",\n",
    "    # Hyperparameter\n",
    "    max_depth=None,\n",
    "    # Hyperparameter\n",
    "    min_samples_split=2,\n",
    "    # Hyperparameter\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    # A good value is m = sqrt(p) based on ISL book\n",
    "    max_features=None,\n",
    "    random_state=None,\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    class_weight=None,\n",
    "    # Hyperparameter\n",
    "    # Greater values of ccp_alpha increase the number of nodes pruned\n",
    "    ccp_alpha=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* High variance (tend to overfit the training data).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples_leaf (Regularizer)\n",
    "\n",
    "* min_samples_split (Regularizer)\n",
    "\n",
    "* max_depth (Pruning)\n",
    "\n",
    "* max_features (Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"ccp_alpha\": [0.0, 0.25, 0.5, 0.75],\n",
    "        \"min_samples_split\": [10, 100, 200],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "BaggingClassifier(\n",
    "    # If None, then the base estimator is a DecisionTreeClassifier\n",
    "    base_estimator=None,\n",
    "    # Number of base estimators in the ensemble\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0,\n",
    "    max_features=1.0,\n",
    "    # Bootstrap samples with replacement\n",
    "    bootstrap=True,\n",
    "    bootstrap_features=False,\n",
    "    # Use out-of-bag samples to estimate the generalization error\n",
    "    oob_score=True,\n",
    "    warm_start=False,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "VotingClassifier(\n",
    "    estimators=[(\"dt\", DecisionTreeClassifier())],\n",
    "    # Hard or soft voting\n",
    "    voting=\"hard\",\n",
    "    weights=None,\n",
    "    n_jobs=-1,\n",
    "    # If voting='soft' and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes)\n",
    "    # If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes)\n",
    "    flatten_transform=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "* If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the `argmax` of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. All classifier must have a `predict_proba` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=\"sqrt\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    bootstrap=True,\n",
    "    # Set to False by default\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    ")\n",
    "\n",
    "# Roughly equivalent to the following\n",
    "BaggingClassifier(\n",
    "    DecisionTreeClassifier(criterion=\"gini\", splitter=\"random\", max_features=\"sqrt\"),\n",
    "    n_estimators=100,\n",
    "    random_state=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners. Trading higher bias (underfitting) for a lower variance (overfitting).\n",
    "\n",
    "* Differs from bagging by introducing more randomness and diversity such that each split only considers a subset of the predictors or features, which can be controlled via `max_features`. Instead of searching for the very best feature among all features when splitting a node, it searches for the best feature among a random subset of features.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* `RandomForestClassifer` has most of the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown) and all the hyperparameters of a `BaggingClassifier` to control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"ccp_alpha\": [0.0, 0.25, 0.5, 0.75],\n",
    "        \"min_samples_split\": [10, 100, 200],\n",
    "        \"max_samples\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ExtraTreesClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion=\"gini\",\n",
    "    max_depth=None,\n",
    "    min_samples_split=2,\n",
    "    min_samples_leaf=1,\n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    max_features=\"sqrt\",\n",
    "    max_leaf_nodes=None,\n",
    "    min_impurity_decrease=0.0,\n",
    "    # Changed default values from False to True\n",
    "    bootstrap=True,\n",
    "    oob_score=True,\n",
    "    n_jobs=-1,\n",
    "    random_state=None,\n",
    "    verbose=0,\n",
    "    warm_start=False,\n",
    "    class_weight=None,\n",
    "    ccp_alpha=0.0,\n",
    "    max_samples=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Even more randomness and diversity than `RandomForestClassifier` by using random thresholds for each feature rather than searching for the best possible thresholds at each split.\n",
    "\n",
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {\n",
    "        \"n_estimators\": [100, 300, 500],\n",
    "        \"max_features\": [\"sqrt\", \"log2\"],\n",
    "        \"ccp_alpha\": [0.0, 0.25, 0.5, 0.75],\n",
    "        \"min_samples_split\": [10, 100, 200],\n",
    "        \"max_samples\": [0.5, 0.75, 1.0],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "AdaBoostClassifier(\n",
    "    # Base esitmator where sample weighting is required, as well as proper classes_ and n_classes_ attributes\n",
    "    # If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1\n",
    "    base_estimator=None,\n",
    "    # In case of perfect fit, the learning procedure is stopped early\n",
    "    n_estimators=500,\n",
    "    # Higher learng rate penalizes weak learners with higher error rates more and rewards stronger learners with lower error rates more\n",
    "    learning_rate=1.0,\n",
    "    # The base_estimator must support calculation of class probabilities with 'SAMME.R', and 'SAMME' uses the SAMME discrete boosting algorithm\n",
    "    algorithm=\"SAMME.R\",\n",
    "    random_state=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Depending on the base estimator used, inputs may need to be standardized.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* base_estimator\n",
    "\n",
    "* n_estimators (controlling underfitting (high bias) or overderfitting (high variance) trade-off)\n",
    "\n",
    "* learning_rate (how much should worst weak learners be penalized and stronger learners to be rewarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "parameter = [\n",
    "    {\n",
    "        \"base_estimator\": [\n",
    "            SVC(kernel=\"rbf\"),\n",
    "            DecisionTreeClassifier(max_depth=3),\n",
    "            LinearSVC(),\n",
    "        ],\n",
    "        \"n_estimators\": [500, 1000],\n",
    "        # Values that Friedman (2001) used in his simulation study\n",
    "        \"learning_rate\": [[1.0, 0.5, 0.25, 0.125, 0.06]],\n",
    "    }\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('python_for_machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
