{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-Learn Estimators\n",
    "\n",
    "## Table of Content \n",
    "\n",
    "- [Regression](#regression)\n",
    "  - [Distance Based](#distance-based)\n",
    "    - [Linear Regression](#linear-regression)\n",
    "  - [Tree Based](#tree-based)\n",
    "    - [Decision Tree Regressor](#decision-tree-regressor)\n",
    "    - [Voting Regressor](#voting-regressor)\n",
    "    - [Bagging Regressor](#bagging-regressor)\n",
    "    - [Random Forest Regressor](#random-forest-regressor)\n",
    "    - [Extra Tree Regressor](#extra-tree-regressor)\n",
    "    - [AdaBoost Regressor](#adaboost-regressor)\n",
    "- [Classification](#classification)\n",
    "  - [Tree Based](#tree-based)\n",
    "    - [Decision Tree Classifier](#decision-tree-classifier)\n",
    "    - [Bagging Classifier](#bagging-classifier)\n",
    "    - [Voting Classifier](#voting-classifier)\n",
    "    - [Random Forest Classifier](#random-forest-classifier)\n",
    "    - [Extra Tree Classifier](#extra-tree-classifier)\n",
    "    - [AdaBoost Classifier](#adaboost-classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Based\n",
    "\n",
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LinearRegression(\n",
    "    fit_intercept=True, \n",
    "    normalize='deprecated', \n",
    "    copy_X=True, \n",
    "    n_jobs=-1, \n",
    "    positive=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Requires standardization (subtract mean and divide by standard deviation). Based on Kutner, M. H., Nachtsheim, C., Neter, J. Applied linear statistical models.\n",
    "\n",
    "* Multicollinearity- when predictor variables are correlated, the regression coefficient of any one variable depends on which other predictor variables are included and which ones are left out. The common interpretation of a regression coefficient as measuring the change in the expected value of the response variable when the given predictor is increased by one unit while all other predictor variables are held constant is not fully applicable.\n",
    "\n",
    "* Sensitive to outliers, influential observations:\n",
    "\n",
    "    * Outlying target values (studentized deleted residuals, studentized residuals)\n",
    "  \n",
    "    * Outlying X observations (hat matrix, leverage values)\n",
    "\n",
    "    * Influential cases (DFFITS, Cook's Distance, DFBETAS)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "DecisionTreeRegressor(\n",
    "    criterion='squared_error', \n",
    "    splitter='best', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    # A good value is m = sqrt(p) based on ISL book\n",
    "    max_features=None, \n",
    "    random_state=None, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    ccp_alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* High variance (tend to overfit the training data).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples_leaf\n",
    "\n",
    "* min_samples_split\n",
    "\n",
    "* max_depth \n",
    "\n",
    "* max_features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'max_features': ['sqrt', 'log2'], 'ccp_alpha': [0.0, 0.25, 0.5, 0.75], 'min_samples_split': [10, 100, 200]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "BaggingRegressor(\n",
    "    # If None, then the base estimator is a DecisionTreeRegressor\n",
    "    base_estimator=None, \n",
    "    # Number of base estimators in the ensemble\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0, \n",
    "    max_features=1.0,\n",
    "    # Bootstrap samples with replacement\n",
    "    bootstrap=True, \n",
    "    bootstrap_features=False,\n",
    "    # Use out-of-bag samples to estimate the generalization error\n",
    "    oob_score=True, \n",
    "    warm_start=False,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingRegressor\n",
    "\n",
    "VotingRegressor(\n",
    "    # List of (str, estimator) tuples\n",
    "    estimators=[('lr', LinearRegression()), ('dt', DecisionTreeRegressor())],\n",
    "    weights=None, \n",
    "    n_jobs=-1, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "* Generally taken out towards the end of the project once there are a few strong candidate models. A voting regressor is an ensemble meta-estimator that fits several base regressors, each on the whole dataset. Then it averages the individual predictions to form a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "RandomForestRegressor(\n",
    "    n_estimators=100, \n",
    "    criterion='squared_error', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features=1.0, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    # Set to False by default\n",
    "    oob_score=True, \n",
    "    # Change to -1 to use all cores\n",
    "    n_jobs=-1, \n",
    "    # Controls bootstrapping of the samples when creating trees and getting a random subsets of features to search for the best feature for splitting\n",
    "    random_state=None, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "# Roughly equivalent to the following\n",
    "BaggingRegressor(\n",
    "    DecisionTreeRegressor(criterion='mse', splitter='random', max_features=1.0),\n",
    "    n_estimators=100, \n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners. Trading higher bias (underfitting) for a lower variance (overfitting).\n",
    "\n",
    "* Differs from bagging by introducing more randomness and diversity such that each split only considers a subset of the predictors or features, which can be controlled via `max_features`. Instead of searching for the very best feature among all features when splitting a node, it searches for the best feature among a random subset of features.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* `RandomForestRegressor` has most of the hyperparameters of a `DecisionTreeRegressor` (to control how trees are grown) and all the hyperparameters of a `BaggingRegressor` to control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'n_estimators': [100, 300, 500],\n",
    "     'max_features': ['sqrt', 'log2'], \n",
    "     'ccp_alpha': [0.0, 0.25, 0.5, 0.75], \n",
    "     'min_samples_split': [10, 100, 200], \n",
    "     'max_samples': [0.5, 0.75, 1.0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "ExtraTreesRegressor(\n",
    "    n_estimators=100, \n",
    "    criterion='squared_error', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features=1.0, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    # Changed default values from False to True\n",
    "    bootstrap=True, \n",
    "    oob_score=True, \n",
    "    n_jobs=-1, \n",
    "    random_state=None, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Even more randomness and diversity than `RandomForestClassifier` by using random thresholds for each feature rather than searching for the best possible thresholds at each split.\n",
    "\n",
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'n_estimators': [100, 300, 500],\n",
    "     'max_features': ['sqrt', 'log2'], \n",
    "     'ccp_alpha': [0.0, 0.25, 0.5, 0.75], \n",
    "     'min_samples_split': [10, 100, 200], \n",
    "     'max_samples': [0.5, 0.75, 1.0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "AdaBoostRegressor(\n",
    "    # Base esitmator where sample weighting is required, as well as proper classes_ and n_classes_ attributes\n",
    "    # If None, then a DecisionTreeRegressor initialized with max_depth=3\n",
    "    base_estimator=None, \n",
    "    # In case of perfect fit, the learning procedure is stopped early\n",
    "    n_estimators=500, \n",
    "    # Higher learng rate penalizes weak learners with higher error rates more and rewards stronger learners with lower error rates more\n",
    "    learning_rate=1.0, \n",
    "    loss='linear', \n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Depending on the base estimator used, inputs may need to be standardized.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* base_estimator\n",
    "\n",
    "* n_estimators (controlling underfitting (high bias) or overderfitting (high variance) trade-off)\n",
    "\n",
    "* learning_rate (how much should worst weak learners be penalized and stronger learners to be rewarded)\n",
    "\n",
    "* loss (the loss function to use when updating the weights after each boosting iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR, LinearSVR\n",
    "\n",
    "parameter = [\n",
    "    {'base_estimator': [SVR(kernel='rbf'), LinearSVR()],\n",
    "     'n_estimators': [100, 500, 1000],\n",
    "     'loss': ['linear', 'square', 'exponential'],\n",
    "     'learning_rate': [1, 5, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Based\n",
    "\n",
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "DecisionTreeRegressor(\n",
    "    criterion='gini', \n",
    "    splitter='best', \n",
    "    # Hyperparameter\n",
    "    max_depth=None, \n",
    "    # Hyperparameter\n",
    "    min_samples_split=2, \n",
    "    # Hyperparameter\n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0,\n",
    "    # A good value is m = sqrt(p) based on ISL book\n",
    "    max_features=None, \n",
    "    random_state=None, \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    class_weight=None, \n",
    "    # Hyperparameter\n",
    "    ccp_alpha=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* High variance (tend to overfit the training data).\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* min_samples_leaf (Regularizer)\n",
    "\n",
    "* min_samples_split (Regularizer)\n",
    "\n",
    "* max_depth (Pruning)\n",
    "\n",
    "* max_features (Pruning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'max_features': ['sqrt', 'log2'], 'ccp_alpha': [0.0, 0.25, 0.5, 0.75], 'min_samples_split': [10, 100, 200]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "BaggingClassifier(\n",
    "    # If None, then the base estimator is a DecisionTreeClassifier\n",
    "    base_estimator=None, \n",
    "    # Number of base estimators in the ensemble\n",
    "    n_estimators=10,\n",
    "    max_samples=1.0, \n",
    "    max_features=1.0,\n",
    "    # Bootstrap samples with replacement\n",
    "    bootstrap=True, \n",
    "    bootstrap_features=False,\n",
    "    # Use out-of-bag samples to estimate the generalization error\n",
    "    oob_score=True, \n",
    "    warm_start=False,\n",
    "    random_state=None,\n",
    "    n_jobs=-1,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Voting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "VotingClassifier(\n",
    "    estimators=[('dt', DecisionTreeClassifier())],\n",
    "    # Hard or soft voting\n",
    "    voting='hard', \n",
    "    weights=None, \n",
    "    n_jobs=-1, \n",
    "    # If voting='soft' and flatten_transform=True, transform method returns matrix with shape (n_samples, n_classifiers * n_classes)\n",
    "    # If flatten_transform=False, it returns (n_classifiers, n_samples, n_classes)\n",
    "    flatten_transform=True, \n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirement\n",
    "\n",
    "* If ‘hard’, uses predicted class labels for majority rule voting. Else if ‘soft’, predicts the class label based on the `argmax` of the sums of the predicted probabilities, which is recommended for an ensemble of well-calibrated classifiers. All classifier must have a `predict_proba` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    criterion='gini', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt', \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    bootstrap=True, \n",
    "    # Set to False by default\n",
    "    oob_score=True, \n",
    "    n_jobs=-1, \n",
    "    random_state=None, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    class_weight=None, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None\n",
    ")\n",
    "\n",
    "# Roughly equivalent to the following\n",
    "BaggingClassifier(\n",
    "    DecisionTreeClassifier(criterion='gini', splitter='random', max_features='sqrt'),\n",
    "    n_estimators=100, \n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Reduce variance of individual weak learners. Trading higher bias (underfitting) for a lower variance (overfitting).\n",
    "\n",
    "* Differs from bagging by introducing more randomness and diversity such that each split only considers a subset of the predictors or features, which can be controlled via `max_features`. Instead of searching for the very best feature among all features when splitting a node, it searches for the best feature among a random subset of features.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* `RandomForestClassifer` has most of the hyperparameters of a `DecisionTreeClassifier` (to control how trees are grown) and all the hyperparameters of a `BaggingClassifier` to control the ensemble itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'n_estimators': [100, 300, 500],\n",
    "     'max_features': ['sqrt', 'log2'], \n",
    "     'ccp_alpha': [0.0, 0.25, 0.5, 0.75], \n",
    "     'min_samples_split': [10, 100, 200], \n",
    "     'max_samples': [0.5, 0.75, 1.0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ExtraTreesClassifier(\n",
    "    n_estimators=100, \n",
    "    criterion='gini', \n",
    "    max_depth=None, \n",
    "    min_samples_split=2, \n",
    "    min_samples_leaf=1, \n",
    "    min_weight_fraction_leaf=0.0, \n",
    "    max_features='sqrt', \n",
    "    max_leaf_nodes=None, \n",
    "    min_impurity_decrease=0.0, \n",
    "    # Changed default values from False to True\n",
    "    bootstrap=True, \n",
    "    oob_score=True, \n",
    "    n_jobs=-1, \n",
    "    random_state=None, \n",
    "    verbose=0, \n",
    "    warm_start=False, \n",
    "    class_weight=None, \n",
    "    ccp_alpha=0.0, \n",
    "    max_samples=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* No need to standardize.\n",
    "\n",
    "* Even more randomness and diversity than `RandomForestClassifier` by using random thresholds for each feature rather than searching for the best possible thresholds at each split.\n",
    "\n",
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = [\n",
    "    {'n_estimators': [100, 300, 500],\n",
    "     'max_features': ['sqrt', 'log2'], \n",
    "     'ccp_alpha': [0.0, 0.25, 0.5, 0.75], \n",
    "     'min_samples_split': [10, 100, 200], \n",
    "     'max_samples': [0.5, 0.75, 1.0]}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoost Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "AdaBoostClassifier(\n",
    "    # Base esitmator where sample weighting is required, as well as proper classes_ and n_classes_ attributes\n",
    "    # If None, then the base estimator is DecisionTreeClassifier initialized with max_depth=1\n",
    "    base_estimator=None, \n",
    "    # In case of perfect fit, the learning procedure is stopped early\n",
    "    n_estimators=500, \n",
    "    # Higher learng rate penalizes weak learners with higher error rates more and rewards stronger learners with lower error rates more\n",
    "    learning_rate=1.0, \n",
    "    # The base_estimator must support calculation of class probabilities with 'SAMME.R', and 'SAMME' uses the SAMME discrete boosting algorithm\n",
    "    algorithm='SAMME.R', \n",
    "    random_state=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "\n",
    "* Depending on the base estimator used, inputs may need to be standardized.\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "* base_estimator\n",
    "\n",
    "* n_estimators (controlling underfitting (high bias) or overderfitting (high variance) trade-off)\n",
    "\n",
    "* learning_rate (how much should worst weak learners be penalized and stronger learners to be rewarded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "parameter = [\n",
    "    {'base_estimator': [SVC(kernel='rbf'), DecisionTreeClassifier(max_depth=3), LinearSVC()],\n",
    "     'n_estimators': [100, 500, 1000],\n",
    "     'learning_rate': [1, 5, 10]}\n",
    "]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "40fc6ebffc74793621f684cf09d9f3d0a501c91440a6f462aebac8d38ed47133"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 ('python_for_machine_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
