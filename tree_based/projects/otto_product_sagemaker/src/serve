#!/opt/conda/envs/rapids/bin/python

import glob
import json
import logging
import os
import sys
import time
import traceback
from functools import lru_cache
from typing import List, Dict, Any, Union, Tuple

import flask
import joblib
import numpy as np
import pandas as pd
from flask import Flask, Response
from sklearn.ensemble import StackingClassifier

def serve() -> None:
    """
    Flask Inference Server for SageMaker hosting of StackingClassifier Model.
    """
    app = Flask(__name__)
    logging.basicConfig(level=logging.DEBUG)

    app.logger.info(f'{os.cpu_count()} CPUs detected')

    @app.route('/ping', methods=['GET'])
    def ping() -> Response:
        """
        SageMaker required method, ping heartbeat.

        Returns
        -------
        Response
            HTTP response with status 200.
        """
        return Response(response='\n', status=200)

    @lru_cache()
    def load_trained_model() -> Tuple[StackingClassifier, str]:
        """
        Cached loading of trained StackingClassifier model into memory.

        Returns
        -------
        Tuple[StackingClassifier, str]
            Tuple containing the reloaded model and its filename.
        """
        model_filename = '/opt/ml/model/stacking_ensemble_model.joblib'
        app.logger.info(f'Loading model : {model_filename}')

        start_time = time.perf_counter()

        reloaded_model = joblib.load(model_filename)

        exec_time = time.perf_counter() - start_time
        app.logger.info(f'Model {model_filename} loaded in {exec_time:.5f}s')

        return reloaded_model, model_filename

    @app.route('/invocations', methods=['POST'])
    def predict() -> Response:
        """
        Run CPU inference on input data, called every time an incoming request arrives.

        Returns
        -------
        Response
            HTTP response with status 200 and prediction results.
        """
        try:
            data_list = json.loads(flask.request.get_data().decode('utf-8'))

            # If the data_list is not a list of lists (i.e., it's a single row), make it a list of lists
            if not isinstance(data_list[0], list):
                data_list = [data_list]

            col_names = [f'feat_{i}' for i in range(1, 94)]

            input_data = pd.DataFrame(data_list, columns=col_names)

        except Exception:
            return Response(
                response='Unable to parse the input data',
                status=415,
                mimetype='text/csv',
            )

        # Cached [reloading] of trained model to process incoming requests
        reloaded_model, model_filename = load_trained_model()

        try:
            start_time = time.perf_counter()
            app.logger.info(f'Running inference using the StackingClassifier model: {model_filename}')

            # Predictions are a numpy array of probabilities with shape (n_samples, n_classes)
            predictions = reloaded_model.predict_proba(input_data)

            # Convert numpy array to list of lists and serialize as JSON formatted string
            predictions_str = json.dumps(predictions.tolist())

            exec_time = time.perf_counter() - start_time
            app.logger.info(f'Inference finished in {exec_time:.5f}s')

            # Return predictions as a string
            return Response(
                response=predictions_str, status=200, mimetype='text/csv'
            )

        # Error during inference
        except Exception as inference_error:
            app.logger.error(inference_error)
            return Response(
                response=f'Inference failure: {inference_error}', status=400, mimetype='text/csv'
            )

    # Initial [non-cached] reload of trained model
    reloaded_model, model_filename = load_trained_model()

    # Trigger start of Flask app
    app.run(host='0.0.0.0', port=8080)


if __name__ == '__main__':

    try:
        serve()
        sys.exit(0)  # Success exit code

    except Exception:
        
        traceback.print_exc()
        sys.exit(-1) # Failure exit code